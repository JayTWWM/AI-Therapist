{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CjWvnaQUrZmD"
   },
   "source": [
    "# Emotion classification using the RAVDESS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ldtHMhuLrewK"
   },
   "source": [
    "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) is licensed under CC BY-NA-SC 4.0. and can be downloaded free of charge at https://zenodo.org/record/1188976.\n",
    "\n",
    "***Construction and Validation***\n",
    "\n",
    "Construction and validation of the RAVDESS is described in our paper: Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391.\n",
    "\n",
    "The RAVDESS contains 7356 files. Each file was rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained adult research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity, interrater reliability, and test-retest intrarater reliability were reported. Validation data is open-access, and can be downloaded along with our paper from PLOS ONE.\n",
    "\n",
    "***Description***\n",
    "\n",
    "The dataset contains the complete set of 7356 RAVDESS files (total size: 24.8 GB). Each of the 24 actors consists of three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound).  Note, there are no song files for Actor_18.\n",
    "\n",
    "***Data***\n",
    "\n",
    "For this task, I have used 4948 samples from the RAVDESS dataset.\n",
    "\n",
    "The samples comes from:\n",
    "\n",
    "- Audio-only files;\n",
    "- Video + audio files: I have extracted the audio from each file using the script Mp4ToWav.py that you can find in the main directory of the project.\n",
    "\n",
    "***License information***\n",
    "\n",
    "The RAVDESS is released under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License, CC BY-NA-SC 4.0\n",
    "\n",
    "***File naming convention***\n",
    "\n",
    "Each of the 7356 RAVDESS files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics:\n",
    "\n",
    "***Filename identifiers***\n",
    "\n",
    "- Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "- Vocal channel (01 = speech, 02 = song).\n",
    "- Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "- Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the ‘neutral’ emotion.\n",
    "- Statement (01 = “Kids are talking by the door”, 02 = “Dogs are sitting by the door”).\n",
    "- Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "- Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
    "\n",
    "Filename example: 02-01-06-01-02-01-12.mp4 \n",
    "\n",
    "- Video-only (02)\n",
    "- Speech (01)\n",
    "- Fearful (06)\n",
    "- Normal intensity (01)\n",
    "- Statement “dogs” (02)\n",
    "- 1st Repetition (01)\n",
    "- 12th Actor (12)\n",
    "- Female, as the actor ID number is even."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JDNbxj45rkvB"
   },
   "source": [
    "# Analysis\n",
    "\n",
    "We are using Colab, a Google Cloud environment for jupyter, so we need to import our files from Google Drive and then install LibROSA, a python package for music and audio analysis.\n",
    "\n",
    "After the import, we will plot the signal of the first file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "N-o2JI49WBAe",
    "outputId": "525ce03d-66ff-4a83-862f-d7657e6be384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "EgFwaDhMbJVm",
    "outputId": "d1f5d32b-177d-4858-c366-d43a6b8783ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (0.6.3)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (4.4.0)\n",
      "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.13.2)\n",
      "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.12.0)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.21.3)\n",
      "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.16.4)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (2.1.8)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.3.0)\n",
      "Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.40.1)\n",
      "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.2.1)\n",
      "Requirement already satisfied: llvmlite>=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (0.29.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rxI4xzngdS-e"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "from librosa import display\n",
    "\n",
    "data, sampling_rate = librosa.load('/content/drive/My Drive/Ravdess/03-01-01-01-01-01-01.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "id": "WgaSHtCIdtX2",
    "outputId": "26d30e6f-28da-47a1-dc5c-bc6ed294a3ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['display']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x7f6d04277668>"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtUAAAEKCAYAAADdM6kMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "% pylab inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob \n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.waveplot(data, sr=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vCtNuVWlr5jL"
   },
   "source": [
    "# Load all files\n",
    "\n",
    "We will create our numpy array extracting Mel-frequency cepstral coefficients (MFCCs), while the classes to predict will be extracted from the name of the file (see the introductory section of this notebook to see the naming convention of the files of this dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AKvuF--gd6F-",
    "outputId": "4fbbbdc4-3bce-47b3-812c-1dd9e3938159"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data loaded. Loading time: 2788.0900292396545 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "path = '/content/drive/My Drive/Ravdess/'\n",
    "lst = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "  for file in files:\n",
    "      try:\n",
    "        #Load librosa array, obtain mfcss, store the file and the mcss information in a new array\n",
    "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
    "        # The instruction below converts the labels (from 1 to 8) to a series from 0 to 7\n",
    "        # This is because our predictor needs to start from 0 otherwise it will try to predict also 0.\n",
    "        file = int(file[7:8]) - 1 \n",
    "        arr = mfccs, file\n",
    "        lst.append(arr)\n",
    "      # If the file is not valid, skip it\n",
    "      except ValueError:\n",
    "        continue\n",
    "\n",
    "print(\"--- Data loaded. Loading time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kLSggnF7kKY1"
   },
   "outputs": [],
   "source": [
    "# Creating X and y: zip makes a list of all the first elements, and a list of all the second elements.\n",
    "X, y = zip(*lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VzvBRTJIlIE9",
    "outputId": "6eb806ec-f065-4420-d526-c1fa8d00c26c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4948, 40), (4948,))"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xOutQiAlCjOY"
   },
   "outputs": [],
   "source": [
    "# Saving joblib files to not load them again with the loop above\n",
    "\n",
    "import joblib\n",
    "\n",
    "X_name = 'X.joblib'\n",
    "y_name = 'y.joblib'\n",
    "save_dir = '/content/drive/My Drive/Ravdess_model'\n",
    "\n",
    "savedX = joblib.dump(X, os.path.join(save_dir, X_name))\n",
    "savedy = joblib.dump(y, os.path.join(save_dir, y_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nIoFdycUXMxA"
   },
   "outputs": [],
   "source": [
    "# Loading saved models\n",
    "\n",
    "X = joblib.load('/content/drive/My Drive/Ravdess_model/X.joblib')\n",
    "y = joblib.load('/content/drive/My Drive/Ravdess_model/y.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Agw-3KN1sDhh"
   },
   "source": [
    "# Decision Tree Classifier\n",
    "\n",
    "To make a first attempt in accomplishing this classification task I chose a decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q-Xgb5NslTBO"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UshLOC1ClWL3"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_BnCR52nlXw0"
   },
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "qWyTownblZM0",
    "outputId": "61708923-f907-442b-86d7-b3e5b2840c29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HEuw6TUQlr7C"
   },
   "outputs": [],
   "source": [
    "predictions = dtree.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1v0i0V7sMw7"
   },
   "source": [
    "Let's go with our classification report.\n",
    "\n",
    "Before we start, a quick reminder of the classes we are trying to predict:\n",
    "\n",
    "emotions = {\n",
    "    \"neutral\": \"0\",\n",
    "    \"calm\": \"1\",\n",
    "    \"happy\": \"2\",\n",
    "    \"sad\": \"3\",\n",
    "    \"angry\": \"4\", \n",
    "    \"fearful\": \"5\", \n",
    "    \"disgust\": \"6\", \n",
    "    \"surprised\": \"7\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "c4kNSYkAleIv",
    "outputId": "fb407f71-b7de-4a15-cee9-527a69d45c11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.81      0.77       134\n",
      "           1       0.87      0.84      0.85       251\n",
      "           2       0.82      0.71      0.76       242\n",
      "           3       0.74      0.73      0.73       271\n",
      "           4       0.83      0.84      0.84       253\n",
      "           5       0.75      0.84      0.79       239\n",
      "           6       0.75      0.72      0.74       127\n",
      "           7       0.73      0.78      0.75       116\n",
      "\n",
      "    accuracy                           0.79      1633\n",
      "   macro avg       0.78      0.78      0.78      1633\n",
      "weighted avg       0.79      0.79      0.78      1633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lCVgjLj-gwE2"
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jfaTxzZ1w__y"
   },
   "source": [
    "In this second approach, I switched to a random forest classifier and I made a gridsearch to make some hyperparameters tuning.\n",
    "\n",
    "The gridsearch is not shown in the code below otherwise the notebook will require too much time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wcov_DCXgs7v"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3eo0ljqzg-KM"
   },
   "outputs": [],
   "source": [
    "rforest = RandomForestClassifier(criterion=\"gini\", max_depth=10, max_features=\"log2\", \n",
    "                                 max_leaf_nodes = 100, min_samples_leaf = 3, min_samples_split = 20, \n",
    "                                 n_estimators= 22000, random_state= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Tg45qSOfg-26",
    "outputId": "c31df1c1-9342-4485-b7f8-211cc4077e7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=10, max_features='log2', max_leaf_nodes=100,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=3, min_samples_split=20,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=22000,\n",
       "                       n_jobs=None, oob_score=False, random_state=5, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rforest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aM8KU3qxhGBM"
   },
   "outputs": [],
   "source": [
    "predictions = rforest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "296FW5sBdanI",
    "outputId": "a9fbfcc2-f9c5-4a3d-9bc0-2a6f513ba609"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.53      0.69       134\n",
      "           1       0.66      0.96      0.78       251\n",
      "           2       0.86      0.71      0.78       242\n",
      "           3       0.79      0.64      0.71       271\n",
      "           4       0.89      0.87      0.88       253\n",
      "           5       0.70      0.80      0.75       239\n",
      "           6       0.74      0.57      0.65       127\n",
      "           7       0.59      0.79      0.68       116\n",
      "\n",
      "    accuracy                           0.76      1633\n",
      "   macro avg       0.78      0.74      0.74      1633\n",
      "weighted avg       0.78      0.76      0.75      1633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t9eqMHV3S8i6"
   },
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-QscoyMxQtn"
   },
   "source": [
    "Let's build our neural network!\n",
    "\n",
    "To do so, we need to expand the dimensions of our array, adding a third one using the numpy \"expand_dims\" feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W4i187-Pe-w5"
   },
   "outputs": [],
   "source": [
    "x_traincnn = np.expand_dims(X_train, axis=2)\n",
    "x_testcnn = np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vnvoCRX1gQCh",
    "outputId": "cc9d5f67-0c48-443c-e6b1-6798d200529e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3315, 40, 1), (1633, 40, 1))"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_traincnn.shape, x_testcnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "HZOGIpuefCd3",
    "outputId": "4fe2802a-3147-4725-d79b-3c9cbe993e16"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0809 13:04:02.475065 140107182061440 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0809 13:04:02.535090 140107182061440 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0809 13:04:02.546353 140107182061440 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0809 13:04:02.615960 140107182061440 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0809 13:04:02.629877 140107182061440 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0809 13:04:02.652721 140107182061440 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(128, 5,padding='same',\n",
    "                 input_shape=(40,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(8))\n",
    "model.add(Activation('softmax'))\n",
    "opt = keras.optimizers.rmsprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LphftMIZzUvz"
   },
   "source": [
    "With *model.summary* we can see a recap of what we have build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "pIWPB4Zgfic7",
    "outputId": "49b5d344-637a-452e-c730-76f5471ea889"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 40, 128)           768       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5, 128)            82048     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 5128      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 87,944\n",
      "Trainable params: 87,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5qQSBeBhzcLu"
   },
   "source": [
    "Now we can compile and fit our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "iNI1znbsfpTx",
    "outputId": "872a1dbe-5206-4d6c-a7ce-943b585f4a9e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0809 13:04:02.763562 140107182061440 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0809 13:04:02.780975 140107182061440 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ktdF-nJKfq6F",
    "outputId": "a05f0852-1564-4425-8885-bcea35dd0e8f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0809 13:04:03.107668 140107182061440 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3315 samples, validate on 1633 samples\n",
      "Epoch 1/1000\n",
      "3315/3315 [==============================] - 3s 814us/step - loss: 7.1409 - acc: 0.1490 - val_loss: 2.4472 - val_acc: 0.1310\n",
      "Epoch 2/1000\n",
      "3315/3315 [==============================] - 2s 529us/step - loss: 5.6130 - acc: 0.1593 - val_loss: 2.5236 - val_acc: 0.1806\n",
      "Epoch 3/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 4.3967 - acc: 0.1707 - val_loss: 2.3030 - val_acc: 0.2333\n",
      "Epoch 4/1000\n",
      "3315/3315 [==============================] - 2s 527us/step - loss: 3.4773 - acc: 0.1695 - val_loss: 2.4763 - val_acc: 0.1464\n",
      "Epoch 5/1000\n",
      "3315/3315 [==============================] - 2s 525us/step - loss: 2.7133 - acc: 0.1828 - val_loss: 1.8271 - val_acc: 0.2768\n",
      "Epoch 6/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 2.2937 - acc: 0.2112 - val_loss: 1.9824 - val_acc: 0.1464\n",
      "Epoch 7/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 2.0900 - acc: 0.2422 - val_loss: 1.8515 - val_acc: 0.3399\n",
      "Epoch 8/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 1.9760 - acc: 0.2428 - val_loss: 1.8306 - val_acc: 0.3344\n",
      "Epoch 9/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 1.9258 - acc: 0.2498 - val_loss: 1.8145 - val_acc: 0.3423\n",
      "Epoch 10/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 1.8698 - acc: 0.2733 - val_loss: 1.7932 - val_acc: 0.2848\n",
      "Epoch 11/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 1.8308 - acc: 0.2884 - val_loss: 1.7366 - val_acc: 0.3601\n",
      "Epoch 12/1000\n",
      "3315/3315 [==============================] - 2s 524us/step - loss: 1.8097 - acc: 0.2974 - val_loss: 1.7275 - val_acc: 0.3552\n",
      "Epoch 13/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 1.7807 - acc: 0.3101 - val_loss: 1.6960 - val_acc: 0.3662\n",
      "Epoch 14/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 1.7600 - acc: 0.3240 - val_loss: 1.6885 - val_acc: 0.3570\n",
      "Epoch 15/1000\n",
      "3315/3315 [==============================] - 2s 526us/step - loss: 1.7211 - acc: 0.3382 - val_loss: 1.6423 - val_acc: 0.4250\n",
      "Epoch 16/1000\n",
      "3315/3315 [==============================] - 2s 529us/step - loss: 1.6910 - acc: 0.3427 - val_loss: 1.6295 - val_acc: 0.3907\n",
      "Epoch 17/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 1.6704 - acc: 0.3704 - val_loss: 1.6019 - val_acc: 0.4189\n",
      "Epoch 18/1000\n",
      "3315/3315 [==============================] - 2s 526us/step - loss: 1.6645 - acc: 0.3563 - val_loss: 1.5947 - val_acc: 0.4146\n",
      "Epoch 19/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 1.6310 - acc: 0.3822 - val_loss: 1.5714 - val_acc: 0.4372\n",
      "Epoch 20/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 1.6173 - acc: 0.3840 - val_loss: 1.5419 - val_acc: 0.4421\n",
      "Epoch 21/1000\n",
      "3315/3315 [==============================] - 2s 550us/step - loss: 1.5949 - acc: 0.3922 - val_loss: 1.5250 - val_acc: 0.4630\n",
      "Epoch 22/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 1.5814 - acc: 0.3973 - val_loss: 1.5130 - val_acc: 0.4544\n",
      "Epoch 23/1000\n",
      "3315/3315 [==============================] - 2s 529us/step - loss: 1.5696 - acc: 0.4087 - val_loss: 1.5214 - val_acc: 0.4562\n",
      "Epoch 24/1000\n",
      "3315/3315 [==============================] - 2s 525us/step - loss: 1.5456 - acc: 0.4121 - val_loss: 1.4954 - val_acc: 0.4838\n",
      "Epoch 25/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 1.5282 - acc: 0.4407 - val_loss: 1.5076 - val_acc: 0.4568\n",
      "Epoch 26/1000\n",
      "3315/3315 [==============================] - 2s 525us/step - loss: 1.5234 - acc: 0.4332 - val_loss: 1.4572 - val_acc: 0.4746\n",
      "Epoch 27/1000\n",
      "3315/3315 [==============================] - 2s 523us/step - loss: 1.5070 - acc: 0.4332 - val_loss: 1.4478 - val_acc: 0.5021\n",
      "Epoch 28/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 1.4855 - acc: 0.4534 - val_loss: 1.4388 - val_acc: 0.4985\n",
      "Epoch 29/1000\n",
      "3315/3315 [==============================] - 2s 511us/step - loss: 1.4864 - acc: 0.4434 - val_loss: 1.4157 - val_acc: 0.4923\n",
      "Epoch 30/1000\n",
      "3315/3315 [==============================] - 2s 507us/step - loss: 1.4659 - acc: 0.4474 - val_loss: 1.4351 - val_acc: 0.4679\n",
      "Epoch 31/1000\n",
      "3315/3315 [==============================] - 2s 520us/step - loss: 1.4481 - acc: 0.4682 - val_loss: 1.3921 - val_acc: 0.5132\n",
      "Epoch 32/1000\n",
      "3315/3315 [==============================] - 2s 521us/step - loss: 1.4350 - acc: 0.4688 - val_loss: 1.3926 - val_acc: 0.4991\n",
      "Epoch 33/1000\n",
      "3315/3315 [==============================] - 2s 521us/step - loss: 1.4309 - acc: 0.4609 - val_loss: 1.3769 - val_acc: 0.5150\n",
      "Epoch 34/1000\n",
      "3315/3315 [==============================] - 2s 526us/step - loss: 1.4218 - acc: 0.4827 - val_loss: 1.3823 - val_acc: 0.4832\n",
      "Epoch 35/1000\n",
      "3315/3315 [==============================] - 2s 526us/step - loss: 1.3851 - acc: 0.4866 - val_loss: 1.3669 - val_acc: 0.5113\n",
      "Epoch 36/1000\n",
      "3315/3315 [==============================] - 2s 527us/step - loss: 1.3998 - acc: 0.4887 - val_loss: 1.3477 - val_acc: 0.5095\n",
      "Epoch 37/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 1.3983 - acc: 0.4787 - val_loss: 1.3407 - val_acc: 0.5193\n",
      "Epoch 38/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 1.3730 - acc: 0.4848 - val_loss: 1.3466 - val_acc: 0.5126\n",
      "Epoch 39/1000\n",
      "3315/3315 [==============================] - 2s 529us/step - loss: 1.3666 - acc: 0.4944 - val_loss: 1.3281 - val_acc: 0.5211\n",
      "Epoch 40/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 1.3457 - acc: 0.5062 - val_loss: 1.3023 - val_acc: 0.5315\n",
      "Epoch 41/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 1.3400 - acc: 0.5056 - val_loss: 1.3085 - val_acc: 0.5383\n",
      "Epoch 42/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 1.3369 - acc: 0.5143 - val_loss: 1.2859 - val_acc: 0.5383\n",
      "Epoch 43/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 1.3251 - acc: 0.5089 - val_loss: 1.3067 - val_acc: 0.5187\n",
      "Epoch 44/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 1.3201 - acc: 0.5173 - val_loss: 1.2869 - val_acc: 0.5334\n",
      "Epoch 45/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 1.2995 - acc: 0.5228 - val_loss: 1.2755 - val_acc: 0.5438\n",
      "Epoch 46/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 1.3050 - acc: 0.5315 - val_loss: 1.2565 - val_acc: 0.5462\n",
      "Epoch 47/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 1.2967 - acc: 0.5158 - val_loss: 1.2598 - val_acc: 0.5419\n",
      "Epoch 48/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 1.2948 - acc: 0.5297 - val_loss: 1.2600 - val_acc: 0.5505\n",
      "Epoch 49/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 1.2675 - acc: 0.5345 - val_loss: 1.2647 - val_acc: 0.5389\n",
      "Epoch 50/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 1.2704 - acc: 0.5385 - val_loss: 1.2340 - val_acc: 0.5609\n",
      "Epoch 51/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 1.2566 - acc: 0.5421 - val_loss: 1.2354 - val_acc: 0.5468\n",
      "Epoch 52/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 1.2560 - acc: 0.5385 - val_loss: 1.2446 - val_acc: 0.5493\n",
      "Epoch 53/1000\n",
      "3315/3315 [==============================] - 2s 530us/step - loss: 1.2432 - acc: 0.5460 - val_loss: 1.2227 - val_acc: 0.5517\n",
      "Epoch 54/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 1.2471 - acc: 0.5475 - val_loss: 1.2174 - val_acc: 0.5548\n",
      "Epoch 55/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 1.2253 - acc: 0.5541 - val_loss: 1.2220 - val_acc: 0.5419\n",
      "Epoch 56/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 1.2235 - acc: 0.5557 - val_loss: 1.2052 - val_acc: 0.5615\n",
      "Epoch 57/1000\n",
      "3315/3315 [==============================] - 2s 530us/step - loss: 1.2259 - acc: 0.5508 - val_loss: 1.2094 - val_acc: 0.5615\n",
      "Epoch 58/1000\n",
      "3315/3315 [==============================] - 2s 530us/step - loss: 1.2222 - acc: 0.5481 - val_loss: 1.2389 - val_acc: 0.5628\n",
      "Epoch 59/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 1.2139 - acc: 0.5548 - val_loss: 1.2143 - val_acc: 0.5585\n",
      "Epoch 60/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 1.1999 - acc: 0.5599 - val_loss: 1.1916 - val_acc: 0.5689\n",
      "Epoch 61/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 1.2057 - acc: 0.5707 - val_loss: 1.1815 - val_acc: 0.5775\n",
      "Epoch 62/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 1.1921 - acc: 0.5551 - val_loss: 1.1811 - val_acc: 0.5695\n",
      "Epoch 63/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 1.1795 - acc: 0.5638 - val_loss: 1.1691 - val_acc: 0.5799\n",
      "Epoch 64/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 1.1892 - acc: 0.5668 - val_loss: 1.1691 - val_acc: 0.5787\n",
      "Epoch 65/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 1.1682 - acc: 0.5704 - val_loss: 1.1540 - val_acc: 0.5830\n",
      "Epoch 66/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 1.1695 - acc: 0.5750 - val_loss: 1.1594 - val_acc: 0.5824\n",
      "Epoch 67/1000\n",
      "3315/3315 [==============================] - 2s 547us/step - loss: 1.1598 - acc: 0.5756 - val_loss: 1.1317 - val_acc: 0.6001\n",
      "Epoch 68/1000\n",
      "3315/3315 [==============================] - 2s 548us/step - loss: 1.1672 - acc: 0.5774 - val_loss: 1.1427 - val_acc: 0.5946\n",
      "Epoch 69/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 1.1406 - acc: 0.5840 - val_loss: 1.1308 - val_acc: 0.5824\n",
      "Epoch 70/1000\n",
      "3315/3315 [==============================] - 2s 528us/step - loss: 1.1469 - acc: 0.5783 - val_loss: 1.1454 - val_acc: 0.5891\n",
      "Epoch 71/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 1.1425 - acc: 0.5804 - val_loss: 1.1048 - val_acc: 0.6081\n",
      "Epoch 72/1000\n",
      "3315/3315 [==============================] - 2s 529us/step - loss: 1.1274 - acc: 0.5873 - val_loss: 1.1313 - val_acc: 0.6020\n",
      "Epoch 73/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 1.1380 - acc: 0.5759 - val_loss: 1.1179 - val_acc: 0.5989\n",
      "Epoch 74/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 1.1293 - acc: 0.5837 - val_loss: 1.1274 - val_acc: 0.5891\n",
      "Epoch 75/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 1.1089 - acc: 0.5910 - val_loss: 1.1028 - val_acc: 0.5971\n",
      "Epoch 76/1000\n",
      "3315/3315 [==============================] - 2s 551us/step - loss: 1.1264 - acc: 0.5879 - val_loss: 1.1205 - val_acc: 0.5952\n",
      "Epoch 77/1000\n",
      "3315/3315 [==============================] - 2s 549us/step - loss: 1.1103 - acc: 0.5991 - val_loss: 1.0964 - val_acc: 0.6050\n",
      "Epoch 78/1000\n",
      "3315/3315 [==============================] - 2s 530us/step - loss: 1.1041 - acc: 0.5931 - val_loss: 1.0948 - val_acc: 0.6118\n",
      "Epoch 79/1000\n",
      "3315/3315 [==============================] - 2s 529us/step - loss: 1.0939 - acc: 0.5970 - val_loss: 1.0817 - val_acc: 0.6154\n",
      "Epoch 80/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 1.1019 - acc: 0.6033 - val_loss: 1.0883 - val_acc: 0.6124\n",
      "Epoch 81/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 1.0962 - acc: 0.6000 - val_loss: 1.1082 - val_acc: 0.5934\n",
      "Epoch 82/1000\n",
      "3315/3315 [==============================] - 2s 549us/step - loss: 1.0807 - acc: 0.6106 - val_loss: 1.0705 - val_acc: 0.6124\n",
      "Epoch 83/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 1.0781 - acc: 0.6072 - val_loss: 1.0755 - val_acc: 0.6148\n",
      "Epoch 84/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 1.0788 - acc: 0.6036 - val_loss: 1.0663 - val_acc: 0.6167\n",
      "Epoch 85/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 1.0822 - acc: 0.6142 - val_loss: 1.0759 - val_acc: 0.6105\n",
      "Epoch 86/1000\n",
      "3315/3315 [==============================] - 2s 530us/step - loss: 1.0769 - acc: 0.6069 - val_loss: 1.0652 - val_acc: 0.6124\n",
      "Epoch 87/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 1.0575 - acc: 0.6118 - val_loss: 1.0482 - val_acc: 0.6209\n",
      "Epoch 88/1000\n",
      "3315/3315 [==============================] - 2s 547us/step - loss: 1.0642 - acc: 0.6036 - val_loss: 1.0690 - val_acc: 0.6160\n",
      "Epoch 89/1000\n",
      "3315/3315 [==============================] - 2s 586us/step - loss: 1.0630 - acc: 0.6087 - val_loss: 1.0521 - val_acc: 0.6167\n",
      "Epoch 90/1000\n",
      "3315/3315 [==============================] - 2s 580us/step - loss: 1.0524 - acc: 0.6109 - val_loss: 1.0465 - val_acc: 0.6301\n",
      "Epoch 91/1000\n",
      "3315/3315 [==============================] - 2s 586us/step - loss: 1.0453 - acc: 0.6256 - val_loss: 1.0538 - val_acc: 0.6148\n",
      "Epoch 92/1000\n",
      "3315/3315 [==============================] - 2s 550us/step - loss: 1.0337 - acc: 0.6247 - val_loss: 1.0327 - val_acc: 0.6381\n",
      "Epoch 93/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 1.0426 - acc: 0.6205 - val_loss: 1.0378 - val_acc: 0.6203\n",
      "Epoch 94/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 1.0311 - acc: 0.6332 - val_loss: 1.0388 - val_acc: 0.6320\n",
      "Epoch 95/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 1.0324 - acc: 0.6223 - val_loss: 1.0344 - val_acc: 0.6314\n",
      "Epoch 96/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 1.0272 - acc: 0.6332 - val_loss: 1.0314 - val_acc: 0.6167\n",
      "Epoch 97/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 1.0235 - acc: 0.6271 - val_loss: 1.0262 - val_acc: 0.6179\n",
      "Epoch 98/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 1.0147 - acc: 0.6244 - val_loss: 1.0185 - val_acc: 0.6344\n",
      "Epoch 99/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 1.0215 - acc: 0.6329 - val_loss: 0.9978 - val_acc: 0.6314\n",
      "Epoch 100/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 1.0168 - acc: 0.6320 - val_loss: 0.9998 - val_acc: 0.6448\n",
      "Epoch 101/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 1.0056 - acc: 0.6383 - val_loss: 0.9935 - val_acc: 0.6503\n",
      "Epoch 102/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.9944 - acc: 0.6428 - val_loss: 1.0077 - val_acc: 0.6301\n",
      "Epoch 103/1000\n",
      "3315/3315 [==============================] - 2s 547us/step - loss: 0.9998 - acc: 0.6311 - val_loss: 0.9842 - val_acc: 0.6503\n",
      "Epoch 104/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.9818 - acc: 0.6419 - val_loss: 1.0030 - val_acc: 0.6283\n",
      "Epoch 105/1000\n",
      "3315/3315 [==============================] - 2s 553us/step - loss: 0.9819 - acc: 0.6446 - val_loss: 1.0268 - val_acc: 0.6240\n",
      "Epoch 106/1000\n",
      "3315/3315 [==============================] - 2s 551us/step - loss: 0.9700 - acc: 0.6395 - val_loss: 1.0169 - val_acc: 0.6332\n",
      "Epoch 107/1000\n",
      "3315/3315 [==============================] - 2s 554us/step - loss: 0.9844 - acc: 0.6386 - val_loss: 0.9802 - val_acc: 0.6405\n",
      "Epoch 108/1000\n",
      "3315/3315 [==============================] - 2s 548us/step - loss: 0.9813 - acc: 0.6413 - val_loss: 0.9789 - val_acc: 0.6442\n",
      "Epoch 109/1000\n",
      "3315/3315 [==============================] - 2s 547us/step - loss: 0.9725 - acc: 0.6449 - val_loss: 0.9876 - val_acc: 0.6454\n",
      "Epoch 110/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.9687 - acc: 0.6452 - val_loss: 0.9666 - val_acc: 0.6509\n",
      "Epoch 111/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.9644 - acc: 0.6513 - val_loss: 0.9708 - val_acc: 0.6448\n",
      "Epoch 112/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.9430 - acc: 0.6552 - val_loss: 0.9685 - val_acc: 0.6454\n",
      "Epoch 113/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.9391 - acc: 0.6600 - val_loss: 0.9508 - val_acc: 0.6583\n",
      "Epoch 114/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.9553 - acc: 0.6558 - val_loss: 0.9757 - val_acc: 0.6485\n",
      "Epoch 115/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.9576 - acc: 0.6452 - val_loss: 0.9521 - val_acc: 0.6565\n",
      "Epoch 116/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.9520 - acc: 0.6516 - val_loss: 0.9657 - val_acc: 0.6387\n",
      "Epoch 117/1000\n",
      "3315/3315 [==============================] - 2s 520us/step - loss: 0.9516 - acc: 0.6621 - val_loss: 0.9585 - val_acc: 0.6375\n",
      "Epoch 118/1000\n",
      "3315/3315 [==============================] - 2s 527us/step - loss: 0.9392 - acc: 0.6618 - val_loss: 0.9625 - val_acc: 0.6558\n",
      "Epoch 119/1000\n",
      "3315/3315 [==============================] - 2s 521us/step - loss: 0.9342 - acc: 0.6633 - val_loss: 0.9415 - val_acc: 0.6650\n",
      "Epoch 120/1000\n",
      "3315/3315 [==============================] - 2s 520us/step - loss: 0.9387 - acc: 0.6573 - val_loss: 0.9446 - val_acc: 0.6589\n",
      "Epoch 121/1000\n",
      "3315/3315 [==============================] - 2s 549us/step - loss: 0.9240 - acc: 0.6676 - val_loss: 0.9279 - val_acc: 0.6607\n",
      "Epoch 122/1000\n",
      "3315/3315 [==============================] - 2s 554us/step - loss: 0.9186 - acc: 0.6673 - val_loss: 0.9566 - val_acc: 0.6479\n",
      "Epoch 123/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.9245 - acc: 0.6694 - val_loss: 0.9505 - val_acc: 0.6632\n",
      "Epoch 124/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.9091 - acc: 0.6643 - val_loss: 0.9408 - val_acc: 0.6607\n",
      "Epoch 125/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.9030 - acc: 0.6739 - val_loss: 0.9377 - val_acc: 0.6620\n",
      "Epoch 126/1000\n",
      "3315/3315 [==============================] - 2s 551us/step - loss: 0.9124 - acc: 0.6739 - val_loss: 0.9307 - val_acc: 0.6577\n",
      "Epoch 127/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.9075 - acc: 0.6745 - val_loss: 0.9182 - val_acc: 0.6650\n",
      "Epoch 128/1000\n",
      "3315/3315 [==============================] - 2s 529us/step - loss: 0.8980 - acc: 0.6802 - val_loss: 0.9169 - val_acc: 0.6650\n",
      "Epoch 129/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.8911 - acc: 0.6676 - val_loss: 0.9540 - val_acc: 0.6418\n",
      "Epoch 130/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.8927 - acc: 0.6778 - val_loss: 0.9035 - val_acc: 0.6785\n",
      "Epoch 131/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.8829 - acc: 0.6833 - val_loss: 0.9237 - val_acc: 0.6546\n",
      "Epoch 132/1000\n",
      "3315/3315 [==============================] - 2s 578us/step - loss: 0.8902 - acc: 0.6694 - val_loss: 0.9094 - val_acc: 0.6730\n",
      "Epoch 133/1000\n",
      "3315/3315 [==============================] - 2s 585us/step - loss: 0.8865 - acc: 0.6727 - val_loss: 0.9113 - val_acc: 0.6614\n",
      "Epoch 134/1000\n",
      "3315/3315 [==============================] - 2s 585us/step - loss: 0.8897 - acc: 0.6814 - val_loss: 0.9038 - val_acc: 0.6785\n",
      "Epoch 135/1000\n",
      "3315/3315 [==============================] - 2s 564us/step - loss: 0.8859 - acc: 0.6817 - val_loss: 0.9064 - val_acc: 0.6681\n",
      "Epoch 136/1000\n",
      "3315/3315 [==============================] - 2s 564us/step - loss: 0.8738 - acc: 0.6790 - val_loss: 0.9198 - val_acc: 0.6442\n",
      "Epoch 137/1000\n",
      "3315/3315 [==============================] - 2s 556us/step - loss: 0.8662 - acc: 0.6905 - val_loss: 0.8898 - val_acc: 0.6797\n",
      "Epoch 138/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.8540 - acc: 0.6926 - val_loss: 0.9058 - val_acc: 0.6663\n",
      "Epoch 139/1000\n",
      "3315/3315 [==============================] - 2s 526us/step - loss: 0.8521 - acc: 0.6872 - val_loss: 0.8859 - val_acc: 0.6816\n",
      "Epoch 140/1000\n",
      "3315/3315 [==============================] - 2s 523us/step - loss: 0.8753 - acc: 0.6811 - val_loss: 0.9147 - val_acc: 0.6754\n",
      "Epoch 141/1000\n",
      "3315/3315 [==============================] - 2s 521us/step - loss: 0.8646 - acc: 0.6836 - val_loss: 0.9071 - val_acc: 0.6663\n",
      "Epoch 142/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.8500 - acc: 0.6956 - val_loss: 0.8963 - val_acc: 0.6705\n",
      "Epoch 143/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 0.8573 - acc: 0.6914 - val_loss: 0.9037 - val_acc: 0.6656\n",
      "Epoch 144/1000\n",
      "3315/3315 [==============================] - 2s 527us/step - loss: 0.8509 - acc: 0.6851 - val_loss: 0.8710 - val_acc: 0.6779\n",
      "Epoch 145/1000\n",
      "3315/3315 [==============================] - 2s 529us/step - loss: 0.8441 - acc: 0.6974 - val_loss: 0.8811 - val_acc: 0.6761\n",
      "Epoch 146/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.8480 - acc: 0.6929 - val_loss: 0.8983 - val_acc: 0.6614\n",
      "Epoch 147/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.8309 - acc: 0.7032 - val_loss: 0.8777 - val_acc: 0.6712\n",
      "Epoch 148/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 0.8356 - acc: 0.7056 - val_loss: 0.8927 - val_acc: 0.6754\n",
      "Epoch 149/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.8382 - acc: 0.6986 - val_loss: 0.8909 - val_acc: 0.6846\n",
      "Epoch 150/1000\n",
      "3315/3315 [==============================] - 2s 521us/step - loss: 0.8225 - acc: 0.7002 - val_loss: 0.8579 - val_acc: 0.6791\n",
      "Epoch 151/1000\n",
      "3315/3315 [==============================] - 2s 525us/step - loss: 0.8159 - acc: 0.7056 - val_loss: 0.8900 - val_acc: 0.6779\n",
      "Epoch 152/1000\n",
      "3315/3315 [==============================] - 2s 523us/step - loss: 0.8155 - acc: 0.7002 - val_loss: 0.8797 - val_acc: 0.6730\n",
      "Epoch 153/1000\n",
      "3315/3315 [==============================] - 2s 528us/step - loss: 0.8189 - acc: 0.7050 - val_loss: 0.8468 - val_acc: 0.6963\n",
      "Epoch 154/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.8080 - acc: 0.7092 - val_loss: 0.8456 - val_acc: 0.6871\n",
      "Epoch 155/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.8117 - acc: 0.7056 - val_loss: 0.8602 - val_acc: 0.6859\n",
      "Epoch 156/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.8067 - acc: 0.7122 - val_loss: 0.8721 - val_acc: 0.6767\n",
      "Epoch 157/1000\n",
      "3315/3315 [==============================] - 2s 546us/step - loss: 0.8079 - acc: 0.7089 - val_loss: 0.8657 - val_acc: 0.6785\n",
      "Epoch 158/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.8218 - acc: 0.6980 - val_loss: 0.8512 - val_acc: 0.6834\n",
      "Epoch 159/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.8028 - acc: 0.7095 - val_loss: 0.8297 - val_acc: 0.6975\n",
      "Epoch 160/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.8091 - acc: 0.7083 - val_loss: 0.8119 - val_acc: 0.7067\n",
      "Epoch 161/1000\n",
      "3315/3315 [==============================] - 2s 550us/step - loss: 0.8138 - acc: 0.7077 - val_loss: 0.8354 - val_acc: 0.7030\n",
      "Epoch 162/1000\n",
      "3315/3315 [==============================] - 2s 553us/step - loss: 0.8106 - acc: 0.7062 - val_loss: 0.8429 - val_acc: 0.6932\n",
      "Epoch 163/1000\n",
      "3315/3315 [==============================] - 2s 552us/step - loss: 0.7931 - acc: 0.7137 - val_loss: 0.8538 - val_acc: 0.6901\n",
      "Epoch 164/1000\n",
      "3315/3315 [==============================] - 2s 547us/step - loss: 0.7983 - acc: 0.7098 - val_loss: 0.8183 - val_acc: 0.7091\n",
      "Epoch 165/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.7945 - acc: 0.7113 - val_loss: 0.8287 - val_acc: 0.6932\n",
      "Epoch 166/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.7872 - acc: 0.7098 - val_loss: 0.8302 - val_acc: 0.7073\n",
      "Epoch 167/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.7760 - acc: 0.7249 - val_loss: 0.8116 - val_acc: 0.7018\n",
      "Epoch 168/1000\n",
      "3315/3315 [==============================] - 2s 546us/step - loss: 0.7813 - acc: 0.7164 - val_loss: 0.8345 - val_acc: 0.7030\n",
      "Epoch 169/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.7719 - acc: 0.7225 - val_loss: 0.8105 - val_acc: 0.7048\n",
      "Epoch 170/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.7632 - acc: 0.7240 - val_loss: 0.8454 - val_acc: 0.6883\n",
      "Epoch 171/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.7699 - acc: 0.7237 - val_loss: 0.8408 - val_acc: 0.6969\n",
      "Epoch 172/1000\n",
      "3315/3315 [==============================] - 2s 550us/step - loss: 0.7646 - acc: 0.7246 - val_loss: 0.8162 - val_acc: 0.7055\n",
      "Epoch 173/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.7652 - acc: 0.7243 - val_loss: 0.8029 - val_acc: 0.7048\n",
      "Epoch 174/1000\n",
      "3315/3315 [==============================] - 2s 549us/step - loss: 0.7609 - acc: 0.7210 - val_loss: 0.8122 - val_acc: 0.6938\n",
      "Epoch 175/1000\n",
      "3315/3315 [==============================] - 2s 546us/step - loss: 0.7595 - acc: 0.7186 - val_loss: 0.8094 - val_acc: 0.7012\n",
      "Epoch 176/1000\n",
      "3315/3315 [==============================] - 2s 548us/step - loss: 0.7486 - acc: 0.7312 - val_loss: 0.8089 - val_acc: 0.7134\n",
      "Epoch 177/1000\n",
      "3315/3315 [==============================] - 2s 552us/step - loss: 0.7476 - acc: 0.7403 - val_loss: 0.8292 - val_acc: 0.6963\n",
      "Epoch 178/1000\n",
      "3315/3315 [==============================] - 2s 550us/step - loss: 0.7453 - acc: 0.7333 - val_loss: 0.7917 - val_acc: 0.7122\n",
      "Epoch 179/1000\n",
      "3315/3315 [==============================] - 2s 552us/step - loss: 0.7400 - acc: 0.7282 - val_loss: 0.7990 - val_acc: 0.7018\n",
      "Epoch 180/1000\n",
      "3315/3315 [==============================] - 2s 551us/step - loss: 0.7430 - acc: 0.7318 - val_loss: 0.8076 - val_acc: 0.7048\n",
      "Epoch 181/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.7429 - acc: 0.7370 - val_loss: 0.7977 - val_acc: 0.7103\n",
      "Epoch 182/1000\n",
      "3315/3315 [==============================] - 2s 556us/step - loss: 0.7445 - acc: 0.7306 - val_loss: 0.7949 - val_acc: 0.7159\n",
      "Epoch 183/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.7459 - acc: 0.7309 - val_loss: 0.7877 - val_acc: 0.7128\n",
      "Epoch 184/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.7409 - acc: 0.7327 - val_loss: 0.7883 - val_acc: 0.7140\n",
      "Epoch 185/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.7315 - acc: 0.7360 - val_loss: 0.8089 - val_acc: 0.6993\n",
      "Epoch 186/1000\n",
      "3315/3315 [==============================] - 2s 529us/step - loss: 0.7239 - acc: 0.7466 - val_loss: 0.7930 - val_acc: 0.7122\n",
      "Epoch 187/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 0.7119 - acc: 0.7466 - val_loss: 0.7793 - val_acc: 0.7244\n",
      "Epoch 188/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.7188 - acc: 0.7351 - val_loss: 0.7908 - val_acc: 0.7097\n",
      "Epoch 189/1000\n",
      "3315/3315 [==============================] - 2s 547us/step - loss: 0.7246 - acc: 0.7445 - val_loss: 0.7799 - val_acc: 0.7165\n",
      "Epoch 190/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.7140 - acc: 0.7493 - val_loss: 0.7756 - val_acc: 0.7122\n",
      "Epoch 191/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.7164 - acc: 0.7406 - val_loss: 0.7844 - val_acc: 0.7152\n",
      "Epoch 192/1000\n",
      "3315/3315 [==============================] - 2s 546us/step - loss: 0.7254 - acc: 0.7315 - val_loss: 0.8122 - val_acc: 0.7006\n",
      "Epoch 193/1000\n",
      "3315/3315 [==============================] - 2s 552us/step - loss: 0.7056 - acc: 0.7463 - val_loss: 0.7635 - val_acc: 0.7318\n",
      "Epoch 194/1000\n",
      "3315/3315 [==============================] - 2s 552us/step - loss: 0.7101 - acc: 0.7412 - val_loss: 0.7642 - val_acc: 0.7189\n",
      "Epoch 195/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.7125 - acc: 0.7502 - val_loss: 0.7717 - val_acc: 0.7299\n",
      "Epoch 196/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 0.6979 - acc: 0.7554 - val_loss: 0.7923 - val_acc: 0.7018\n",
      "Epoch 197/1000\n",
      "3315/3315 [==============================] - 2s 530us/step - loss: 0.7139 - acc: 0.7424 - val_loss: 0.7787 - val_acc: 0.7208\n",
      "Epoch 198/1000\n",
      "3315/3315 [==============================] - 2s 529us/step - loss: 0.6981 - acc: 0.7566 - val_loss: 0.7810 - val_acc: 0.7165\n",
      "Epoch 199/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.6963 - acc: 0.7469 - val_loss: 0.7647 - val_acc: 0.7208\n",
      "Epoch 200/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.6854 - acc: 0.7560 - val_loss: 0.7568 - val_acc: 0.7238\n",
      "Epoch 201/1000\n",
      "3315/3315 [==============================] - 2s 548us/step - loss: 0.6814 - acc: 0.7538 - val_loss: 0.8096 - val_acc: 0.7048\n",
      "Epoch 202/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.6938 - acc: 0.7514 - val_loss: 0.7521 - val_acc: 0.7238\n",
      "Epoch 203/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.6859 - acc: 0.7566 - val_loss: 0.7514 - val_acc: 0.7293\n",
      "Epoch 204/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.6888 - acc: 0.7557 - val_loss: 0.7625 - val_acc: 0.7244\n",
      "Epoch 205/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.6974 - acc: 0.7469 - val_loss: 0.7368 - val_acc: 0.7385\n",
      "Epoch 206/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.6708 - acc: 0.7548 - val_loss: 0.7508 - val_acc: 0.7367\n",
      "Epoch 207/1000\n",
      "3315/3315 [==============================] - 2s 528us/step - loss: 0.6810 - acc: 0.7590 - val_loss: 0.7705 - val_acc: 0.7165\n",
      "Epoch 208/1000\n",
      "3315/3315 [==============================] - 2s 514us/step - loss: 0.6899 - acc: 0.7412 - val_loss: 0.7471 - val_acc: 0.7250\n",
      "Epoch 209/1000\n",
      "3315/3315 [==============================] - 2s 522us/step - loss: 0.6630 - acc: 0.7698 - val_loss: 0.7313 - val_acc: 0.7410\n",
      "Epoch 210/1000\n",
      "3315/3315 [==============================] - 2s 530us/step - loss: 0.6794 - acc: 0.7566 - val_loss: 0.7404 - val_acc: 0.7330\n",
      "Epoch 211/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 0.6690 - acc: 0.7575 - val_loss: 0.7374 - val_acc: 0.7312\n",
      "Epoch 212/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.6669 - acc: 0.7593 - val_loss: 0.7302 - val_acc: 0.7428\n",
      "Epoch 213/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.6604 - acc: 0.7653 - val_loss: 0.7353 - val_acc: 0.7299\n",
      "Epoch 214/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.6648 - acc: 0.7704 - val_loss: 0.7482 - val_acc: 0.7238\n",
      "Epoch 215/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.6651 - acc: 0.7650 - val_loss: 0.7284 - val_acc: 0.7404\n",
      "Epoch 216/1000\n",
      "3315/3315 [==============================] - 2s 554us/step - loss: 0.6557 - acc: 0.7623 - val_loss: 0.7358 - val_acc: 0.7324\n",
      "Epoch 217/1000\n",
      "3315/3315 [==============================] - 2s 546us/step - loss: 0.6524 - acc: 0.7665 - val_loss: 0.7158 - val_acc: 0.7428\n",
      "Epoch 218/1000\n",
      "3315/3315 [==============================] - 2s 552us/step - loss: 0.6463 - acc: 0.7668 - val_loss: 0.7596 - val_acc: 0.7177\n",
      "Epoch 219/1000\n",
      "3315/3315 [==============================] - 2s 554us/step - loss: 0.6518 - acc: 0.7638 - val_loss: 0.7209 - val_acc: 0.7428\n",
      "Epoch 220/1000\n",
      "3315/3315 [==============================] - 2s 564us/step - loss: 0.6509 - acc: 0.7710 - val_loss: 0.7287 - val_acc: 0.7391\n",
      "Epoch 221/1000\n",
      "3315/3315 [==============================] - 2s 548us/step - loss: 0.6442 - acc: 0.7680 - val_loss: 0.7331 - val_acc: 0.7269\n",
      "Epoch 222/1000\n",
      "3315/3315 [==============================] - 2s 549us/step - loss: 0.6373 - acc: 0.7807 - val_loss: 0.7345 - val_acc: 0.7489\n",
      "Epoch 223/1000\n",
      "3315/3315 [==============================] - 2s 555us/step - loss: 0.6438 - acc: 0.7635 - val_loss: 0.7382 - val_acc: 0.7330\n",
      "Epoch 224/1000\n",
      "3315/3315 [==============================] - 2s 549us/step - loss: 0.6292 - acc: 0.7735 - val_loss: 0.7175 - val_acc: 0.7508\n",
      "Epoch 225/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.6385 - acc: 0.7713 - val_loss: 0.7300 - val_acc: 0.7385\n",
      "Epoch 226/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.6402 - acc: 0.7744 - val_loss: 0.7274 - val_acc: 0.7440\n",
      "Epoch 227/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.6474 - acc: 0.7620 - val_loss: 0.7166 - val_acc: 0.7489\n",
      "Epoch 228/1000\n",
      "3315/3315 [==============================] - 2s 546us/step - loss: 0.6217 - acc: 0.7804 - val_loss: 0.7146 - val_acc: 0.7422\n",
      "Epoch 229/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.6279 - acc: 0.7774 - val_loss: 0.7244 - val_acc: 0.7355\n",
      "Epoch 230/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.6296 - acc: 0.7692 - val_loss: 0.7054 - val_acc: 0.7342\n",
      "Epoch 231/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.6250 - acc: 0.7759 - val_loss: 0.7041 - val_acc: 0.7428\n",
      "Epoch 232/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.6190 - acc: 0.7882 - val_loss: 0.7001 - val_acc: 0.7391\n",
      "Epoch 233/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.6101 - acc: 0.7831 - val_loss: 0.6951 - val_acc: 0.7606\n",
      "Epoch 234/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.6208 - acc: 0.7765 - val_loss: 0.7058 - val_acc: 0.7391\n",
      "Epoch 235/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.6176 - acc: 0.7819 - val_loss: 0.7057 - val_acc: 0.7342\n",
      "Epoch 236/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.6084 - acc: 0.7762 - val_loss: 0.6988 - val_acc: 0.7367\n",
      "Epoch 237/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.6231 - acc: 0.7771 - val_loss: 0.7506 - val_acc: 0.7281\n",
      "Epoch 238/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.6030 - acc: 0.7828 - val_loss: 0.7173 - val_acc: 0.7367\n",
      "Epoch 239/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.6119 - acc: 0.7771 - val_loss: 0.7159 - val_acc: 0.7483\n",
      "Epoch 240/1000\n",
      "3315/3315 [==============================] - 2s 546us/step - loss: 0.5947 - acc: 0.7771 - val_loss: 0.7018 - val_acc: 0.7422\n",
      "Epoch 241/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.5978 - acc: 0.7810 - val_loss: 0.6828 - val_acc: 0.7514\n",
      "Epoch 242/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.5985 - acc: 0.7879 - val_loss: 0.6894 - val_acc: 0.7502\n",
      "Epoch 243/1000\n",
      "3315/3315 [==============================] - 2s 547us/step - loss: 0.6118 - acc: 0.7765 - val_loss: 0.6942 - val_acc: 0.7514\n",
      "Epoch 244/1000\n",
      "3315/3315 [==============================] - 2s 554us/step - loss: 0.6072 - acc: 0.7834 - val_loss: 0.6794 - val_acc: 0.7551\n",
      "Epoch 245/1000\n",
      "3315/3315 [==============================] - 2s 552us/step - loss: 0.5939 - acc: 0.7882 - val_loss: 0.6900 - val_acc: 0.7483\n",
      "Epoch 246/1000\n",
      "3315/3315 [==============================] - 2s 551us/step - loss: 0.5907 - acc: 0.7849 - val_loss: 0.6598 - val_acc: 0.7661\n",
      "Epoch 247/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.5838 - acc: 0.7934 - val_loss: 0.6875 - val_acc: 0.7551\n",
      "Epoch 248/1000\n",
      "3315/3315 [==============================] - 2s 546us/step - loss: 0.5884 - acc: 0.7882 - val_loss: 0.6808 - val_acc: 0.7587\n",
      "Epoch 249/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.5767 - acc: 0.8012 - val_loss: 0.6731 - val_acc: 0.7508\n",
      "Epoch 250/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.5848 - acc: 0.7867 - val_loss: 0.6697 - val_acc: 0.7581\n",
      "Epoch 251/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.5942 - acc: 0.7849 - val_loss: 0.6829 - val_acc: 0.7489\n",
      "Epoch 252/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.5726 - acc: 0.7885 - val_loss: 0.6836 - val_acc: 0.7636\n",
      "Epoch 253/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 0.5692 - acc: 0.7973 - val_loss: 0.6706 - val_acc: 0.7600\n",
      "Epoch 254/1000\n",
      "3315/3315 [==============================] - 2s 529us/step - loss: 0.5736 - acc: 0.8000 - val_loss: 0.6896 - val_acc: 0.7514\n",
      "Epoch 255/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.5711 - acc: 0.7958 - val_loss: 0.6630 - val_acc: 0.7642\n",
      "Epoch 256/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.5743 - acc: 0.7973 - val_loss: 0.6852 - val_acc: 0.7551\n",
      "Epoch 257/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.5836 - acc: 0.7906 - val_loss: 0.6558 - val_acc: 0.7636\n",
      "Epoch 258/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.5651 - acc: 0.7922 - val_loss: 0.6594 - val_acc: 0.7655\n",
      "Epoch 259/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.5685 - acc: 0.7988 - val_loss: 0.6831 - val_acc: 0.7453\n",
      "Epoch 260/1000\n",
      "3315/3315 [==============================] - 2s 572us/step - loss: 0.5599 - acc: 0.8015 - val_loss: 0.6520 - val_acc: 0.7722\n",
      "Epoch 261/1000\n",
      "3315/3315 [==============================] - 2s 589us/step - loss: 0.5497 - acc: 0.8048 - val_loss: 0.6525 - val_acc: 0.7765\n",
      "Epoch 262/1000\n",
      "3315/3315 [==============================] - 2s 604us/step - loss: 0.5755 - acc: 0.7894 - val_loss: 0.6606 - val_acc: 0.7618\n",
      "Epoch 263/1000\n",
      "3315/3315 [==============================] - 2s 568us/step - loss: 0.5495 - acc: 0.8021 - val_loss: 0.6488 - val_acc: 0.7679\n",
      "Epoch 264/1000\n",
      "3315/3315 [==============================] - 2s 547us/step - loss: 0.5612 - acc: 0.8021 - val_loss: 0.6608 - val_acc: 0.7636\n",
      "Epoch 265/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.5486 - acc: 0.8033 - val_loss: 0.6552 - val_acc: 0.7685\n",
      "Epoch 266/1000\n",
      "3315/3315 [==============================] - 2s 549us/step - loss: 0.5585 - acc: 0.8015 - val_loss: 0.6533 - val_acc: 0.7722\n",
      "Epoch 267/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.5470 - acc: 0.8112 - val_loss: 0.6639 - val_acc: 0.7612\n",
      "Epoch 268/1000\n",
      "3315/3315 [==============================] - 2s 549us/step - loss: 0.5602 - acc: 0.8051 - val_loss: 0.6504 - val_acc: 0.7648\n",
      "Epoch 269/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.5519 - acc: 0.8078 - val_loss: 0.6537 - val_acc: 0.7691\n",
      "Epoch 270/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.5640 - acc: 0.8009 - val_loss: 0.6507 - val_acc: 0.7740\n",
      "Epoch 271/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.5510 - acc: 0.8018 - val_loss: 0.6285 - val_acc: 0.7734\n",
      "Epoch 272/1000\n",
      "3315/3315 [==============================] - 2s 547us/step - loss: 0.5508 - acc: 0.7994 - val_loss: 0.6586 - val_acc: 0.7673\n",
      "Epoch 273/1000\n",
      "3315/3315 [==============================] - 2s 549us/step - loss: 0.5397 - acc: 0.8090 - val_loss: 0.6340 - val_acc: 0.7771\n",
      "Epoch 274/1000\n",
      "3315/3315 [==============================] - 2s 552us/step - loss: 0.5492 - acc: 0.8060 - val_loss: 0.6441 - val_acc: 0.7697\n",
      "Epoch 275/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.5295 - acc: 0.8103 - val_loss: 0.6177 - val_acc: 0.7875\n",
      "Epoch 276/1000\n",
      "3315/3315 [==============================] - 2s 555us/step - loss: 0.5400 - acc: 0.8097 - val_loss: 0.6396 - val_acc: 0.7734\n",
      "Epoch 277/1000\n",
      "3315/3315 [==============================] - 2s 559us/step - loss: 0.5273 - acc: 0.8078 - val_loss: 0.6310 - val_acc: 0.7728\n",
      "Epoch 278/1000\n",
      "3315/3315 [==============================] - 2s 554us/step - loss: 0.5412 - acc: 0.8103 - val_loss: 0.6304 - val_acc: 0.7838\n",
      "Epoch 279/1000\n",
      "3315/3315 [==============================] - 2s 556us/step - loss: 0.5240 - acc: 0.8148 - val_loss: 0.6408 - val_acc: 0.7728\n",
      "Epoch 280/1000\n",
      "3315/3315 [==============================] - 2s 552us/step - loss: 0.5290 - acc: 0.8100 - val_loss: 0.7004 - val_acc: 0.7489\n",
      "Epoch 281/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.5230 - acc: 0.8090 - val_loss: 0.6479 - val_acc: 0.7667\n",
      "Epoch 282/1000\n",
      "3315/3315 [==============================] - 2s 550us/step - loss: 0.5237 - acc: 0.8166 - val_loss: 0.6266 - val_acc: 0.7820\n",
      "Epoch 283/1000\n",
      "3315/3315 [==============================] - 2s 549us/step - loss: 0.5123 - acc: 0.8145 - val_loss: 0.6441 - val_acc: 0.7691\n",
      "Epoch 284/1000\n",
      "3315/3315 [==============================] - 2s 563us/step - loss: 0.5284 - acc: 0.8187 - val_loss: 0.6374 - val_acc: 0.7777\n",
      "Epoch 285/1000\n",
      "3315/3315 [==============================] - 2s 549us/step - loss: 0.5168 - acc: 0.8151 - val_loss: 0.6262 - val_acc: 0.7851\n",
      "Epoch 286/1000\n",
      "3315/3315 [==============================] - 2s 546us/step - loss: 0.5343 - acc: 0.8112 - val_loss: 0.6349 - val_acc: 0.7820\n",
      "Epoch 287/1000\n",
      "3315/3315 [==============================] - 2s 550us/step - loss: 0.5131 - acc: 0.8202 - val_loss: 0.6510 - val_acc: 0.7710\n",
      "Epoch 288/1000\n",
      "3315/3315 [==============================] - 2s 551us/step - loss: 0.5128 - acc: 0.8184 - val_loss: 0.6311 - val_acc: 0.7765\n",
      "Epoch 289/1000\n",
      "3315/3315 [==============================] - 2s 561us/step - loss: 0.5124 - acc: 0.8193 - val_loss: 0.6153 - val_acc: 0.7900\n",
      "Epoch 290/1000\n",
      "3315/3315 [==============================] - 2s 552us/step - loss: 0.5054 - acc: 0.8238 - val_loss: 0.6153 - val_acc: 0.7857\n",
      "Epoch 291/1000\n",
      "3315/3315 [==============================] - 2s 558us/step - loss: 0.4983 - acc: 0.8199 - val_loss: 0.6161 - val_acc: 0.7863\n",
      "Epoch 292/1000\n",
      "3315/3315 [==============================] - 2s 567us/step - loss: 0.5128 - acc: 0.8241 - val_loss: 0.6158 - val_acc: 0.7783\n",
      "Epoch 293/1000\n",
      "3315/3315 [==============================] - 2s 555us/step - loss: 0.5174 - acc: 0.8229 - val_loss: 0.6183 - val_acc: 0.7863\n",
      "Epoch 294/1000\n",
      "3315/3315 [==============================] - 2s 551us/step - loss: 0.5035 - acc: 0.8217 - val_loss: 0.6251 - val_acc: 0.7826\n",
      "Epoch 295/1000\n",
      "3315/3315 [==============================] - 2s 551us/step - loss: 0.5062 - acc: 0.8163 - val_loss: 0.6150 - val_acc: 0.7857\n",
      "Epoch 296/1000\n",
      "3315/3315 [==============================] - 2s 557us/step - loss: 0.4974 - acc: 0.8223 - val_loss: 0.5955 - val_acc: 0.7936\n",
      "Epoch 297/1000\n",
      "3315/3315 [==============================] - 2s 551us/step - loss: 0.4934 - acc: 0.8244 - val_loss: 0.6214 - val_acc: 0.7795\n",
      "Epoch 298/1000\n",
      "3315/3315 [==============================] - 2s 553us/step - loss: 0.4987 - acc: 0.8238 - val_loss: 0.6271 - val_acc: 0.7673\n",
      "Epoch 299/1000\n",
      "3315/3315 [==============================] - 2s 551us/step - loss: 0.4946 - acc: 0.8232 - val_loss: 0.6059 - val_acc: 0.7906\n",
      "Epoch 300/1000\n",
      "3315/3315 [==============================] - 2s 561us/step - loss: 0.5039 - acc: 0.8187 - val_loss: 0.6072 - val_acc: 0.7930\n",
      "Epoch 301/1000\n",
      "3315/3315 [==============================] - 2s 555us/step - loss: 0.5082 - acc: 0.8205 - val_loss: 0.6115 - val_acc: 0.7826\n",
      "Epoch 302/1000\n",
      "3315/3315 [==============================] - 2s 546us/step - loss: 0.5073 - acc: 0.8175 - val_loss: 0.6016 - val_acc: 0.7942\n",
      "Epoch 303/1000\n",
      "3315/3315 [==============================] - 2s 548us/step - loss: 0.4836 - acc: 0.8302 - val_loss: 0.6050 - val_acc: 0.7863\n",
      "Epoch 304/1000\n",
      "3315/3315 [==============================] - 2s 575us/step - loss: 0.4819 - acc: 0.8299 - val_loss: 0.6141 - val_acc: 0.7881\n",
      "Epoch 305/1000\n",
      "3315/3315 [==============================] - 2s 584us/step - loss: 0.5007 - acc: 0.8244 - val_loss: 0.6078 - val_acc: 0.7893\n",
      "Epoch 306/1000\n",
      "3315/3315 [==============================] - 2s 587us/step - loss: 0.4829 - acc: 0.8305 - val_loss: 0.5972 - val_acc: 0.7998\n",
      "Epoch 307/1000\n",
      "3315/3315 [==============================] - 2s 584us/step - loss: 0.4901 - acc: 0.8196 - val_loss: 0.5918 - val_acc: 0.7961\n",
      "Epoch 308/1000\n",
      "3315/3315 [==============================] - 2s 579us/step - loss: 0.4836 - acc: 0.8338 - val_loss: 0.5922 - val_acc: 0.7949\n",
      "Epoch 309/1000\n",
      "3315/3315 [==============================] - 2s 582us/step - loss: 0.5002 - acc: 0.8160 - val_loss: 0.5828 - val_acc: 0.7967\n",
      "Epoch 310/1000\n",
      "3315/3315 [==============================] - 2s 548us/step - loss: 0.4894 - acc: 0.8253 - val_loss: 0.5894 - val_acc: 0.7912\n",
      "Epoch 311/1000\n",
      "3315/3315 [==============================] - 2s 558us/step - loss: 0.4752 - acc: 0.8356 - val_loss: 0.5882 - val_acc: 0.7991\n",
      "Epoch 312/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.4744 - acc: 0.8380 - val_loss: 0.5998 - val_acc: 0.8028\n",
      "Epoch 313/1000\n",
      "3315/3315 [==============================] - 2s 556us/step - loss: 0.4841 - acc: 0.8275 - val_loss: 0.5795 - val_acc: 0.7991\n",
      "Epoch 314/1000\n",
      "3315/3315 [==============================] - 2s 555us/step - loss: 0.4804 - acc: 0.8335 - val_loss: 0.5763 - val_acc: 0.7998\n",
      "Epoch 315/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.4907 - acc: 0.8208 - val_loss: 0.5827 - val_acc: 0.7998\n",
      "Epoch 316/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.4753 - acc: 0.8335 - val_loss: 0.5774 - val_acc: 0.8004\n",
      "Epoch 317/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.4735 - acc: 0.8268 - val_loss: 0.5747 - val_acc: 0.8083\n",
      "Epoch 318/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.4751 - acc: 0.8362 - val_loss: 0.5728 - val_acc: 0.7991\n",
      "Epoch 319/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.4686 - acc: 0.8311 - val_loss: 0.5819 - val_acc: 0.8004\n",
      "Epoch 320/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.4569 - acc: 0.8389 - val_loss: 0.5904 - val_acc: 0.8065\n",
      "Epoch 321/1000\n",
      "3315/3315 [==============================] - 2s 549us/step - loss: 0.4731 - acc: 0.8329 - val_loss: 0.5906 - val_acc: 0.7875\n",
      "Epoch 322/1000\n",
      "3315/3315 [==============================] - 2s 565us/step - loss: 0.4671 - acc: 0.8365 - val_loss: 0.5965 - val_acc: 0.7955\n",
      "Epoch 323/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.4597 - acc: 0.8374 - val_loss: 0.5718 - val_acc: 0.8016\n",
      "Epoch 324/1000\n",
      "3315/3315 [==============================] - 2s 549us/step - loss: 0.4686 - acc: 0.8356 - val_loss: 0.5846 - val_acc: 0.7979\n",
      "Epoch 325/1000\n",
      "3315/3315 [==============================] - 2s 551us/step - loss: 0.4584 - acc: 0.8434 - val_loss: 0.5982 - val_acc: 0.7924\n",
      "Epoch 326/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.4543 - acc: 0.8437 - val_loss: 0.5758 - val_acc: 0.7887\n",
      "Epoch 327/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.4709 - acc: 0.8293 - val_loss: 0.5703 - val_acc: 0.8083\n",
      "Epoch 328/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.4625 - acc: 0.8365 - val_loss: 0.5850 - val_acc: 0.8004\n",
      "Epoch 329/1000\n",
      "3315/3315 [==============================] - 2s 529us/step - loss: 0.4518 - acc: 0.8383 - val_loss: 0.5873 - val_acc: 0.7961\n",
      "Epoch 330/1000\n",
      "3315/3315 [==============================] - 2s 547us/step - loss: 0.4497 - acc: 0.8434 - val_loss: 0.5851 - val_acc: 0.7961\n",
      "Epoch 331/1000\n",
      "3315/3315 [==============================] - 2s 555us/step - loss: 0.4600 - acc: 0.8356 - val_loss: 0.5830 - val_acc: 0.7998\n",
      "Epoch 332/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.4511 - acc: 0.8410 - val_loss: 0.5491 - val_acc: 0.8145\n",
      "Epoch 333/1000\n",
      "3315/3315 [==============================] - 2s 557us/step - loss: 0.4461 - acc: 0.8416 - val_loss: 0.5603 - val_acc: 0.8169\n",
      "Epoch 334/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.4529 - acc: 0.8416 - val_loss: 0.5628 - val_acc: 0.8034\n",
      "Epoch 335/1000\n",
      "3315/3315 [==============================] - 2s 546us/step - loss: 0.4406 - acc: 0.8489 - val_loss: 0.5519 - val_acc: 0.8138\n",
      "Epoch 336/1000\n",
      "3315/3315 [==============================] - 2s 548us/step - loss: 0.4388 - acc: 0.8474 - val_loss: 0.5676 - val_acc: 0.8096\n",
      "Epoch 337/1000\n",
      "3315/3315 [==============================] - 2s 549us/step - loss: 0.4447 - acc: 0.8437 - val_loss: 0.5560 - val_acc: 0.8022\n",
      "Epoch 338/1000\n",
      "3315/3315 [==============================] - 2s 550us/step - loss: 0.4508 - acc: 0.8389 - val_loss: 0.5710 - val_acc: 0.8065\n",
      "Epoch 339/1000\n",
      "3315/3315 [==============================] - 2s 549us/step - loss: 0.4326 - acc: 0.8446 - val_loss: 0.5862 - val_acc: 0.7985\n",
      "Epoch 340/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.4678 - acc: 0.8335 - val_loss: 0.5601 - val_acc: 0.8132\n",
      "Epoch 341/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.4416 - acc: 0.8422 - val_loss: 0.5597 - val_acc: 0.8022\n",
      "Epoch 342/1000\n",
      "3315/3315 [==============================] - 2s 558us/step - loss: 0.4246 - acc: 0.8489 - val_loss: 0.5470 - val_acc: 0.8181\n",
      "Epoch 343/1000\n",
      "3315/3315 [==============================] - 2s 553us/step - loss: 0.4429 - acc: 0.8434 - val_loss: 0.5496 - val_acc: 0.8126\n",
      "Epoch 344/1000\n",
      "3315/3315 [==============================] - 2s 547us/step - loss: 0.4251 - acc: 0.8489 - val_loss: 0.5734 - val_acc: 0.7998\n",
      "Epoch 345/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.4290 - acc: 0.8468 - val_loss: 0.5637 - val_acc: 0.8102\n",
      "Epoch 346/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.4272 - acc: 0.8504 - val_loss: 0.5684 - val_acc: 0.8034\n",
      "Epoch 347/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.4264 - acc: 0.8513 - val_loss: 0.5691 - val_acc: 0.7985\n",
      "Epoch 348/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.4344 - acc: 0.8531 - val_loss: 0.5851 - val_acc: 0.7979\n",
      "Epoch 349/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.4227 - acc: 0.8498 - val_loss: 0.5498 - val_acc: 0.8181\n",
      "Epoch 350/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.4176 - acc: 0.8531 - val_loss: 0.5350 - val_acc: 0.8126\n",
      "Epoch 351/1000\n",
      "3315/3315 [==============================] - 2s 530us/step - loss: 0.4399 - acc: 0.8446 - val_loss: 0.5509 - val_acc: 0.8132\n",
      "Epoch 352/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.4479 - acc: 0.8380 - val_loss: 0.5529 - val_acc: 0.8285\n",
      "Epoch 353/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.4366 - acc: 0.8456 - val_loss: 0.5645 - val_acc: 0.8096\n",
      "Epoch 354/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.4232 - acc: 0.8543 - val_loss: 0.5543 - val_acc: 0.8089\n",
      "Epoch 355/1000\n",
      "3315/3315 [==============================] - 2s 551us/step - loss: 0.4269 - acc: 0.8468 - val_loss: 0.5431 - val_acc: 0.8194\n",
      "Epoch 356/1000\n",
      "3315/3315 [==============================] - 2s 547us/step - loss: 0.4086 - acc: 0.8621 - val_loss: 0.5565 - val_acc: 0.8083\n",
      "Epoch 357/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.4285 - acc: 0.8513 - val_loss: 0.5609 - val_acc: 0.8010\n",
      "Epoch 358/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 0.4346 - acc: 0.8492 - val_loss: 0.5764 - val_acc: 0.7930\n",
      "Epoch 359/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.4383 - acc: 0.8452 - val_loss: 0.5532 - val_acc: 0.8071\n",
      "Epoch 360/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.4250 - acc: 0.8501 - val_loss: 0.5459 - val_acc: 0.8083\n",
      "Epoch 361/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.4073 - acc: 0.8570 - val_loss: 0.5317 - val_acc: 0.8089\n",
      "Epoch 362/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.4124 - acc: 0.8618 - val_loss: 0.5355 - val_acc: 0.8200\n",
      "Epoch 363/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.4182 - acc: 0.8513 - val_loss: 0.5232 - val_acc: 0.8200\n",
      "Epoch 364/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.4130 - acc: 0.8492 - val_loss: 0.5451 - val_acc: 0.8132\n",
      "Epoch 365/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.4201 - acc: 0.8555 - val_loss: 0.5426 - val_acc: 0.8157\n",
      "Epoch 366/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.4031 - acc: 0.8567 - val_loss: 0.5436 - val_acc: 0.8163\n",
      "Epoch 367/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.4239 - acc: 0.8486 - val_loss: 0.5314 - val_acc: 0.8187\n",
      "Epoch 368/1000\n",
      "3315/3315 [==============================] - 2s 546us/step - loss: 0.3931 - acc: 0.8621 - val_loss: 0.5282 - val_acc: 0.8218\n",
      "Epoch 369/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.4165 - acc: 0.8516 - val_loss: 0.5288 - val_acc: 0.8255\n",
      "Epoch 370/1000\n",
      "3315/3315 [==============================] - 2s 523us/step - loss: 0.4032 - acc: 0.8570 - val_loss: 0.5254 - val_acc: 0.8322\n",
      "Epoch 371/1000\n",
      "3315/3315 [==============================] - 2s 529us/step - loss: 0.3984 - acc: 0.8630 - val_loss: 0.5464 - val_acc: 0.8163\n",
      "Epoch 372/1000\n",
      "3315/3315 [==============================] - 2s 526us/step - loss: 0.4017 - acc: 0.8591 - val_loss: 0.5314 - val_acc: 0.8230\n",
      "Epoch 373/1000\n",
      "3315/3315 [==============================] - 2s 529us/step - loss: 0.3979 - acc: 0.8597 - val_loss: 0.5152 - val_acc: 0.8138\n",
      "Epoch 374/1000\n",
      "3315/3315 [==============================] - 2s 523us/step - loss: 0.4041 - acc: 0.8576 - val_loss: 0.5179 - val_acc: 0.8249\n",
      "Epoch 375/1000\n",
      "3315/3315 [==============================] - 2s 524us/step - loss: 0.4094 - acc: 0.8567 - val_loss: 0.5151 - val_acc: 0.8328\n",
      "Epoch 376/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 0.4147 - acc: 0.8543 - val_loss: 0.5238 - val_acc: 0.8334\n",
      "Epoch 377/1000\n",
      "3315/3315 [==============================] - 2s 527us/step - loss: 0.3963 - acc: 0.8588 - val_loss: 0.5381 - val_acc: 0.8218\n",
      "Epoch 378/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 0.3985 - acc: 0.8597 - val_loss: 0.5306 - val_acc: 0.8285\n",
      "Epoch 379/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.4095 - acc: 0.8579 - val_loss: 0.5317 - val_acc: 0.8230\n",
      "Epoch 380/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.3943 - acc: 0.8694 - val_loss: 0.5346 - val_acc: 0.8138\n",
      "Epoch 381/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.4015 - acc: 0.8627 - val_loss: 0.5294 - val_acc: 0.8132\n",
      "Epoch 382/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.3882 - acc: 0.8649 - val_loss: 0.5255 - val_acc: 0.8151\n",
      "Epoch 383/1000\n",
      "3315/3315 [==============================] - 2s 527us/step - loss: 0.3977 - acc: 0.8630 - val_loss: 0.5137 - val_acc: 0.8347\n",
      "Epoch 384/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.3964 - acc: 0.8646 - val_loss: 0.5208 - val_acc: 0.8206\n",
      "Epoch 385/1000\n",
      "3315/3315 [==============================] - 2s 522us/step - loss: 0.3767 - acc: 0.8691 - val_loss: 0.5186 - val_acc: 0.8291\n",
      "Epoch 386/1000\n",
      "3315/3315 [==============================] - 2s 512us/step - loss: 0.3771 - acc: 0.8688 - val_loss: 0.5224 - val_acc: 0.8334\n",
      "Epoch 387/1000\n",
      "3315/3315 [==============================] - 2s 520us/step - loss: 0.3926 - acc: 0.8624 - val_loss: 0.5128 - val_acc: 0.8230\n",
      "Epoch 388/1000\n",
      "3315/3315 [==============================] - 2s 519us/step - loss: 0.3801 - acc: 0.8691 - val_loss: 0.5001 - val_acc: 0.8359\n",
      "Epoch 389/1000\n",
      "3315/3315 [==============================] - 2s 517us/step - loss: 0.3741 - acc: 0.8772 - val_loss: 0.5239 - val_acc: 0.8310\n",
      "Epoch 390/1000\n",
      "3315/3315 [==============================] - 2s 521us/step - loss: 0.3798 - acc: 0.8718 - val_loss: 0.5198 - val_acc: 0.8310\n",
      "Epoch 391/1000\n",
      "3315/3315 [==============================] - 2s 526us/step - loss: 0.3903 - acc: 0.8591 - val_loss: 0.5013 - val_acc: 0.8365\n",
      "Epoch 392/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.3811 - acc: 0.8715 - val_loss: 0.5175 - val_acc: 0.8169\n",
      "Epoch 393/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 0.3752 - acc: 0.8745 - val_loss: 0.5086 - val_acc: 0.8279\n",
      "Epoch 394/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 0.3796 - acc: 0.8643 - val_loss: 0.5166 - val_acc: 0.8194\n",
      "Epoch 395/1000\n",
      "3315/3315 [==============================] - 2s 526us/step - loss: 0.3886 - acc: 0.8676 - val_loss: 0.5212 - val_acc: 0.8347\n",
      "Epoch 396/1000\n",
      "3315/3315 [==============================] - 2s 526us/step - loss: 0.3665 - acc: 0.8721 - val_loss: 0.5217 - val_acc: 0.8224\n",
      "Epoch 397/1000\n",
      "3315/3315 [==============================] - 2s 527us/step - loss: 0.3701 - acc: 0.8694 - val_loss: 0.5088 - val_acc: 0.8322\n",
      "Epoch 398/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.3692 - acc: 0.8718 - val_loss: 0.5294 - val_acc: 0.8273\n",
      "Epoch 399/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.3683 - acc: 0.8615 - val_loss: 0.4975 - val_acc: 0.8353\n",
      "Epoch 400/1000\n",
      "3315/3315 [==============================] - 2s 525us/step - loss: 0.3732 - acc: 0.8757 - val_loss: 0.4922 - val_acc: 0.8420\n",
      "Epoch 401/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 0.3808 - acc: 0.8597 - val_loss: 0.5109 - val_acc: 0.8340\n",
      "Epoch 402/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.3715 - acc: 0.8778 - val_loss: 0.4956 - val_acc: 0.8334\n",
      "Epoch 403/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.3711 - acc: 0.8694 - val_loss: 0.4988 - val_acc: 0.8432\n",
      "Epoch 404/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.3675 - acc: 0.8697 - val_loss: 0.4907 - val_acc: 0.8396\n",
      "Epoch 405/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.3673 - acc: 0.8730 - val_loss: 0.4902 - val_acc: 0.8347\n",
      "Epoch 406/1000\n",
      "3315/3315 [==============================] - 2s 527us/step - loss: 0.3554 - acc: 0.8769 - val_loss: 0.5245 - val_acc: 0.8285\n",
      "Epoch 407/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.3652 - acc: 0.8751 - val_loss: 0.4925 - val_acc: 0.8371\n",
      "Epoch 408/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 0.3612 - acc: 0.8772 - val_loss: 0.5131 - val_acc: 0.8322\n",
      "Epoch 409/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.3650 - acc: 0.8718 - val_loss: 0.5040 - val_acc: 0.8402\n",
      "Epoch 410/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.3662 - acc: 0.8769 - val_loss: 0.4986 - val_acc: 0.8298\n",
      "Epoch 411/1000\n",
      "3315/3315 [==============================] - 2s 527us/step - loss: 0.3727 - acc: 0.8733 - val_loss: 0.5034 - val_acc: 0.8273\n",
      "Epoch 412/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.3668 - acc: 0.8727 - val_loss: 0.5095 - val_acc: 0.8291\n",
      "Epoch 413/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.3507 - acc: 0.8772 - val_loss: 0.4957 - val_acc: 0.8359\n",
      "Epoch 414/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.3577 - acc: 0.8793 - val_loss: 0.4994 - val_acc: 0.8334\n",
      "Epoch 415/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.3568 - acc: 0.8730 - val_loss: 0.4975 - val_acc: 0.8420\n",
      "Epoch 416/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 0.3462 - acc: 0.8787 - val_loss: 0.5061 - val_acc: 0.8340\n",
      "Epoch 417/1000\n",
      "3315/3315 [==============================] - 2s 529us/step - loss: 0.3569 - acc: 0.8766 - val_loss: 0.4929 - val_acc: 0.8414\n",
      "Epoch 418/1000\n",
      "3315/3315 [==============================] - 2s 530us/step - loss: 0.3599 - acc: 0.8742 - val_loss: 0.4945 - val_acc: 0.8371\n",
      "Epoch 419/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.3368 - acc: 0.8842 - val_loss: 0.5020 - val_acc: 0.8328\n",
      "Epoch 420/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 0.3501 - acc: 0.8802 - val_loss: 0.5021 - val_acc: 0.8383\n",
      "Epoch 421/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.3539 - acc: 0.8751 - val_loss: 0.4851 - val_acc: 0.8469\n",
      "Epoch 422/1000\n",
      "3315/3315 [==============================] - 2s 528us/step - loss: 0.3501 - acc: 0.8817 - val_loss: 0.4746 - val_acc: 0.8500\n",
      "Epoch 423/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.3450 - acc: 0.8821 - val_loss: 0.5079 - val_acc: 0.8414\n",
      "Epoch 424/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.3344 - acc: 0.8875 - val_loss: 0.4782 - val_acc: 0.8396\n",
      "Epoch 425/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.3666 - acc: 0.8676 - val_loss: 0.4913 - val_acc: 0.8438\n",
      "Epoch 426/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.3367 - acc: 0.8857 - val_loss: 0.4845 - val_acc: 0.8328\n",
      "Epoch 427/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.3341 - acc: 0.8833 - val_loss: 0.4924 - val_acc: 0.8310\n",
      "Epoch 428/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.3320 - acc: 0.8830 - val_loss: 0.5051 - val_acc: 0.8279\n",
      "Epoch 429/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.3463 - acc: 0.8775 - val_loss: 0.4840 - val_acc: 0.8383\n",
      "Epoch 430/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.3358 - acc: 0.8842 - val_loss: 0.4784 - val_acc: 0.8481\n",
      "Epoch 431/1000\n",
      "3315/3315 [==============================] - 2s 549us/step - loss: 0.3393 - acc: 0.8857 - val_loss: 0.4852 - val_acc: 0.8389\n",
      "Epoch 432/1000\n",
      "3315/3315 [==============================] - 2s 589us/step - loss: 0.3460 - acc: 0.8814 - val_loss: 0.4787 - val_acc: 0.8359\n",
      "Epoch 433/1000\n",
      "3315/3315 [==============================] - 2s 581us/step - loss: 0.3385 - acc: 0.8811 - val_loss: 0.4677 - val_acc: 0.8383\n",
      "Epoch 434/1000\n",
      "3315/3315 [==============================] - 2s 566us/step - loss: 0.3506 - acc: 0.8751 - val_loss: 0.4763 - val_acc: 0.8481\n",
      "Epoch 435/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.3311 - acc: 0.8872 - val_loss: 0.5060 - val_acc: 0.8255\n",
      "Epoch 436/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.3396 - acc: 0.8845 - val_loss: 0.4668 - val_acc: 0.8432\n",
      "Epoch 437/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.3231 - acc: 0.8971 - val_loss: 0.4656 - val_acc: 0.8512\n",
      "Epoch 438/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.3577 - acc: 0.8799 - val_loss: 0.4672 - val_acc: 0.8500\n",
      "Epoch 439/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.3309 - acc: 0.8827 - val_loss: 0.4753 - val_acc: 0.8414\n",
      "Epoch 440/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.3260 - acc: 0.8839 - val_loss: 0.4877 - val_acc: 0.8328\n",
      "Epoch 441/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.3378 - acc: 0.8887 - val_loss: 0.4747 - val_acc: 0.8604\n",
      "Epoch 442/1000\n",
      "3315/3315 [==============================] - 2s 530us/step - loss: 0.3228 - acc: 0.8881 - val_loss: 0.5024 - val_acc: 0.8267\n",
      "Epoch 443/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.3296 - acc: 0.8872 - val_loss: 0.4783 - val_acc: 0.8487\n",
      "Epoch 444/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.3255 - acc: 0.8817 - val_loss: 0.4707 - val_acc: 0.8438\n",
      "Epoch 445/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 0.3343 - acc: 0.8793 - val_loss: 0.4746 - val_acc: 0.8512\n",
      "Epoch 446/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.3352 - acc: 0.8848 - val_loss: 0.4670 - val_acc: 0.8445\n",
      "Epoch 447/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.3437 - acc: 0.8830 - val_loss: 0.4714 - val_acc: 0.8494\n",
      "Epoch 448/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.3294 - acc: 0.8869 - val_loss: 0.4842 - val_acc: 0.8451\n",
      "Epoch 449/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.3219 - acc: 0.8833 - val_loss: 0.4452 - val_acc: 0.8598\n",
      "Epoch 450/1000\n",
      "3315/3315 [==============================] - 2s 547us/step - loss: 0.3347 - acc: 0.8839 - val_loss: 0.4611 - val_acc: 0.8494\n",
      "Epoch 451/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.3465 - acc: 0.8778 - val_loss: 0.4631 - val_acc: 0.8536\n",
      "Epoch 452/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.3272 - acc: 0.8920 - val_loss: 0.4644 - val_acc: 0.8463\n",
      "Epoch 453/1000\n",
      "3315/3315 [==============================] - 2s 525us/step - loss: 0.3274 - acc: 0.8869 - val_loss: 0.4800 - val_acc: 0.8426\n",
      "Epoch 454/1000\n",
      "3315/3315 [==============================] - 2s 521us/step - loss: 0.3293 - acc: 0.8878 - val_loss: 0.4661 - val_acc: 0.8524\n",
      "Epoch 455/1000\n",
      "3315/3315 [==============================] - 2s 516us/step - loss: 0.3275 - acc: 0.8899 - val_loss: 0.4542 - val_acc: 0.8628\n",
      "Epoch 456/1000\n",
      "3315/3315 [==============================] - 2s 527us/step - loss: 0.3199 - acc: 0.8896 - val_loss: 0.4965 - val_acc: 0.8414\n",
      "Epoch 457/1000\n",
      "3315/3315 [==============================] - 2s 528us/step - loss: 0.3206 - acc: 0.8938 - val_loss: 0.4615 - val_acc: 0.8543\n",
      "Epoch 458/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 0.3297 - acc: 0.8932 - val_loss: 0.4731 - val_acc: 0.8549\n",
      "Epoch 459/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.3183 - acc: 0.8926 - val_loss: 0.4718 - val_acc: 0.8457\n",
      "Epoch 460/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.3179 - acc: 0.8890 - val_loss: 0.4383 - val_acc: 0.8555\n",
      "Epoch 461/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.3042 - acc: 0.8974 - val_loss: 0.4595 - val_acc: 0.8524\n",
      "Epoch 462/1000\n",
      "3315/3315 [==============================] - 2s 521us/step - loss: 0.3078 - acc: 0.8956 - val_loss: 0.4436 - val_acc: 0.8512\n",
      "Epoch 463/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.3072 - acc: 0.8944 - val_loss: 0.4706 - val_acc: 0.8475\n",
      "Epoch 464/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.3310 - acc: 0.8899 - val_loss: 0.4459 - val_acc: 0.8549\n",
      "Epoch 465/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.3205 - acc: 0.8941 - val_loss: 0.4502 - val_acc: 0.8683\n",
      "Epoch 466/1000\n",
      "3315/3315 [==============================] - 2s 514us/step - loss: 0.3040 - acc: 0.8932 - val_loss: 0.4589 - val_acc: 0.8347\n",
      "Epoch 467/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.3077 - acc: 0.8914 - val_loss: 0.4765 - val_acc: 0.8475\n",
      "Epoch 468/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.3153 - acc: 0.8902 - val_loss: 0.4379 - val_acc: 0.8702\n",
      "Epoch 469/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.3119 - acc: 0.8872 - val_loss: 0.4386 - val_acc: 0.8702\n",
      "Epoch 470/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 0.3183 - acc: 0.8905 - val_loss: 0.4598 - val_acc: 0.8610\n",
      "Epoch 471/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.3270 - acc: 0.8938 - val_loss: 0.4551 - val_acc: 0.8647\n",
      "Epoch 472/1000\n",
      "3315/3315 [==============================] - 2s 528us/step - loss: 0.2969 - acc: 0.8944 - val_loss: 0.4642 - val_acc: 0.8543\n",
      "Epoch 473/1000\n",
      "3315/3315 [==============================] - 2s 528us/step - loss: 0.2904 - acc: 0.9062 - val_loss: 0.4566 - val_acc: 0.8579\n",
      "Epoch 474/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.3064 - acc: 0.8956 - val_loss: 0.4774 - val_acc: 0.8567\n",
      "Epoch 475/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.3065 - acc: 0.8962 - val_loss: 0.4428 - val_acc: 0.8585\n",
      "Epoch 476/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.3103 - acc: 0.8920 - val_loss: 0.4428 - val_acc: 0.8677\n",
      "Epoch 477/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.3100 - acc: 0.8926 - val_loss: 0.4510 - val_acc: 0.8457\n",
      "Epoch 478/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 0.3048 - acc: 0.8935 - val_loss: 0.4433 - val_acc: 0.8641\n",
      "Epoch 479/1000\n",
      "3315/3315 [==============================] - 2s 562us/step - loss: 0.3095 - acc: 0.8899 - val_loss: 0.4499 - val_acc: 0.8573\n",
      "Epoch 480/1000\n",
      "3315/3315 [==============================] - 2s 571us/step - loss: 0.2920 - acc: 0.8974 - val_loss: 0.4563 - val_acc: 0.8518\n",
      "Epoch 481/1000\n",
      "3315/3315 [==============================] - 2s 584us/step - loss: 0.3144 - acc: 0.8929 - val_loss: 0.4484 - val_acc: 0.8622\n",
      "Epoch 482/1000\n",
      "3315/3315 [==============================] - 2s 589us/step - loss: 0.2992 - acc: 0.8959 - val_loss: 0.4411 - val_acc: 0.8561\n",
      "Epoch 483/1000\n",
      "3315/3315 [==============================] - 2s 575us/step - loss: 0.3114 - acc: 0.8971 - val_loss: 0.4506 - val_acc: 0.8616\n",
      "Epoch 484/1000\n",
      "3315/3315 [==============================] - 2s 553us/step - loss: 0.3013 - acc: 0.8908 - val_loss: 0.4372 - val_acc: 0.8696\n",
      "Epoch 485/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.3094 - acc: 0.8914 - val_loss: 0.4359 - val_acc: 0.8647\n",
      "Epoch 486/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.3011 - acc: 0.8947 - val_loss: 0.4550 - val_acc: 0.8598\n",
      "Epoch 487/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.2997 - acc: 0.8920 - val_loss: 0.4463 - val_acc: 0.8585\n",
      "Epoch 488/1000\n",
      "3315/3315 [==============================] - 2s 530us/step - loss: 0.2891 - acc: 0.9005 - val_loss: 0.4504 - val_acc: 0.8555\n",
      "Epoch 489/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.2949 - acc: 0.8986 - val_loss: 0.4492 - val_acc: 0.8585\n",
      "Epoch 490/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.2945 - acc: 0.8965 - val_loss: 0.4373 - val_acc: 0.8604\n",
      "Epoch 491/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.2936 - acc: 0.8971 - val_loss: 0.4457 - val_acc: 0.8543\n",
      "Epoch 492/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.2906 - acc: 0.9077 - val_loss: 0.4297 - val_acc: 0.8659\n",
      "Epoch 493/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.2851 - acc: 0.9032 - val_loss: 0.4473 - val_acc: 0.8610\n",
      "Epoch 494/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.2935 - acc: 0.8995 - val_loss: 0.4280 - val_acc: 0.8696\n",
      "Epoch 495/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.2932 - acc: 0.8944 - val_loss: 0.4461 - val_acc: 0.8647\n",
      "Epoch 496/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.2841 - acc: 0.9005 - val_loss: 0.4507 - val_acc: 0.8506\n",
      "Epoch 497/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.2907 - acc: 0.9023 - val_loss: 0.4352 - val_acc: 0.8659\n",
      "Epoch 498/1000\n",
      "3315/3315 [==============================] - 2s 528us/step - loss: 0.3008 - acc: 0.8986 - val_loss: 0.4288 - val_acc: 0.8690\n",
      "Epoch 499/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.2932 - acc: 0.8995 - val_loss: 0.4401 - val_acc: 0.8690\n",
      "Epoch 500/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 0.2978 - acc: 0.8971 - val_loss: 0.4363 - val_acc: 0.8604\n",
      "Epoch 501/1000\n",
      "3315/3315 [==============================] - 2s 527us/step - loss: 0.2965 - acc: 0.8974 - val_loss: 0.4292 - val_acc: 0.8641\n",
      "Epoch 502/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 0.2916 - acc: 0.9035 - val_loss: 0.4328 - val_acc: 0.8720\n",
      "Epoch 503/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.2852 - acc: 0.9044 - val_loss: 0.4425 - val_acc: 0.8579\n",
      "Epoch 504/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.2920 - acc: 0.8956 - val_loss: 0.4176 - val_acc: 0.8708\n",
      "Epoch 505/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.2826 - acc: 0.9014 - val_loss: 0.4367 - val_acc: 0.8659\n",
      "Epoch 506/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.2813 - acc: 0.9026 - val_loss: 0.4269 - val_acc: 0.8647\n",
      "Epoch 507/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.2753 - acc: 0.9080 - val_loss: 0.4402 - val_acc: 0.8628\n",
      "Epoch 508/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 0.2841 - acc: 0.9047 - val_loss: 0.4210 - val_acc: 0.8714\n",
      "Epoch 509/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.2699 - acc: 0.9104 - val_loss: 0.4480 - val_acc: 0.8702\n",
      "Epoch 510/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.2656 - acc: 0.9113 - val_loss: 0.4439 - val_acc: 0.8653\n",
      "Epoch 511/1000\n",
      "3315/3315 [==============================] - 2s 547us/step - loss: 0.2843 - acc: 0.9068 - val_loss: 0.4478 - val_acc: 0.8512\n",
      "Epoch 512/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.2717 - acc: 0.9056 - val_loss: 0.4265 - val_acc: 0.8726\n",
      "Epoch 513/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.2756 - acc: 0.9071 - val_loss: 0.4214 - val_acc: 0.8683\n",
      "Epoch 514/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 0.2689 - acc: 0.9131 - val_loss: 0.4119 - val_acc: 0.8720\n",
      "Epoch 515/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 0.2714 - acc: 0.9104 - val_loss: 0.4318 - val_acc: 0.8573\n",
      "Epoch 516/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.2595 - acc: 0.9122 - val_loss: 0.4417 - val_acc: 0.8628\n",
      "Epoch 517/1000\n",
      "3315/3315 [==============================] - 2s 548us/step - loss: 0.2823 - acc: 0.9101 - val_loss: 0.4295 - val_acc: 0.8683\n",
      "Epoch 518/1000\n",
      "3315/3315 [==============================] - 2s 559us/step - loss: 0.2600 - acc: 0.9125 - val_loss: 0.4249 - val_acc: 0.8690\n",
      "Epoch 519/1000\n",
      "3315/3315 [==============================] - 2s 546us/step - loss: 0.2809 - acc: 0.9029 - val_loss: 0.4245 - val_acc: 0.8702\n",
      "Epoch 520/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.2668 - acc: 0.9068 - val_loss: 0.4499 - val_acc: 0.8561\n",
      "Epoch 521/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.2825 - acc: 0.9020 - val_loss: 0.4259 - val_acc: 0.8653\n",
      "Epoch 522/1000\n",
      "3315/3315 [==============================] - 2s 548us/step - loss: 0.2665 - acc: 0.9077 - val_loss: 0.4131 - val_acc: 0.8757\n",
      "Epoch 523/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.2788 - acc: 0.9044 - val_loss: 0.4224 - val_acc: 0.8714\n",
      "Epoch 524/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.2798 - acc: 0.9068 - val_loss: 0.4334 - val_acc: 0.8720\n",
      "Epoch 525/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.2787 - acc: 0.9113 - val_loss: 0.4215 - val_acc: 0.8788\n",
      "Epoch 526/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.2762 - acc: 0.9032 - val_loss: 0.4366 - val_acc: 0.8616\n",
      "Epoch 527/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.2795 - acc: 0.9038 - val_loss: 0.4225 - val_acc: 0.8714\n",
      "Epoch 528/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.2802 - acc: 0.9008 - val_loss: 0.4523 - val_acc: 0.8567\n",
      "Epoch 529/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.2711 - acc: 0.9086 - val_loss: 0.4236 - val_acc: 0.8745\n",
      "Epoch 530/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.2793 - acc: 0.9062 - val_loss: 0.4178 - val_acc: 0.8696\n",
      "Epoch 531/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.2696 - acc: 0.9017 - val_loss: 0.4328 - val_acc: 0.8592\n",
      "Epoch 532/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.2676 - acc: 0.9071 - val_loss: 0.4169 - val_acc: 0.8665\n",
      "Epoch 533/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.2801 - acc: 0.9086 - val_loss: 0.4246 - val_acc: 0.8745\n",
      "Epoch 534/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.2637 - acc: 0.9077 - val_loss: 0.4420 - val_acc: 0.8628\n",
      "Epoch 535/1000\n",
      "3315/3315 [==============================] - 2s 547us/step - loss: 0.2551 - acc: 0.9155 - val_loss: 0.4056 - val_acc: 0.8714\n",
      "Epoch 536/1000\n",
      "3315/3315 [==============================] - 2s 548us/step - loss: 0.2676 - acc: 0.9080 - val_loss: 0.4332 - val_acc: 0.8653\n",
      "Epoch 537/1000\n",
      "3315/3315 [==============================] - 2s 529us/step - loss: 0.2697 - acc: 0.9038 - val_loss: 0.4265 - val_acc: 0.8690\n",
      "Epoch 538/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.2552 - acc: 0.9122 - val_loss: 0.4409 - val_acc: 0.8536\n",
      "Epoch 539/1000\n",
      "3315/3315 [==============================] - 2s 548us/step - loss: 0.2683 - acc: 0.9083 - val_loss: 0.4510 - val_acc: 0.8585\n",
      "Epoch 540/1000\n",
      "3315/3315 [==============================] - 2s 546us/step - loss: 0.2695 - acc: 0.9062 - val_loss: 0.4134 - val_acc: 0.8702\n",
      "Epoch 541/1000\n",
      "3315/3315 [==============================] - 2s 549us/step - loss: 0.2535 - acc: 0.9140 - val_loss: 0.4092 - val_acc: 0.8739\n",
      "Epoch 542/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.2709 - acc: 0.9062 - val_loss: 0.4235 - val_acc: 0.8610\n",
      "Epoch 543/1000\n",
      "3315/3315 [==============================] - 2s 520us/step - loss: 0.2572 - acc: 0.9107 - val_loss: 0.4277 - val_acc: 0.8702\n",
      "Epoch 544/1000\n",
      "3315/3315 [==============================] - 2s 525us/step - loss: 0.2574 - acc: 0.9134 - val_loss: 0.4062 - val_acc: 0.8800\n",
      "Epoch 545/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 0.2667 - acc: 0.9140 - val_loss: 0.4195 - val_acc: 0.8659\n",
      "Epoch 546/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 0.2672 - acc: 0.9083 - val_loss: 0.4130 - val_acc: 0.8732\n",
      "Epoch 547/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.2616 - acc: 0.9071 - val_loss: 0.4081 - val_acc: 0.8781\n",
      "Epoch 548/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.2537 - acc: 0.9107 - val_loss: 0.4311 - val_acc: 0.8671\n",
      "Epoch 549/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.2636 - acc: 0.9098 - val_loss: 0.3973 - val_acc: 0.8855\n",
      "Epoch 550/1000\n",
      "3315/3315 [==============================] - 2s 551us/step - loss: 0.2527 - acc: 0.9179 - val_loss: 0.4084 - val_acc: 0.8800\n",
      "Epoch 551/1000\n",
      "3315/3315 [==============================] - 2s 530us/step - loss: 0.2517 - acc: 0.9113 - val_loss: 0.4293 - val_acc: 0.8702\n",
      "Epoch 552/1000\n",
      "3315/3315 [==============================] - 2s 546us/step - loss: 0.2562 - acc: 0.9134 - val_loss: 0.4018 - val_acc: 0.8726\n",
      "Epoch 553/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.2549 - acc: 0.9146 - val_loss: 0.4044 - val_acc: 0.8861\n",
      "Epoch 554/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.2588 - acc: 0.9107 - val_loss: 0.4241 - val_acc: 0.8732\n",
      "Epoch 555/1000\n",
      "3315/3315 [==============================] - 2s 530us/step - loss: 0.2619 - acc: 0.9116 - val_loss: 0.4217 - val_acc: 0.8641\n",
      "Epoch 556/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.2479 - acc: 0.9146 - val_loss: 0.4051 - val_acc: 0.8806\n",
      "Epoch 557/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.2480 - acc: 0.9167 - val_loss: 0.4290 - val_acc: 0.8653\n",
      "Epoch 558/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 0.2414 - acc: 0.9195 - val_loss: 0.4102 - val_acc: 0.8806\n",
      "Epoch 559/1000\n",
      "3315/3315 [==============================] - 2s 546us/step - loss: 0.2584 - acc: 0.9122 - val_loss: 0.3983 - val_acc: 0.8745\n",
      "Epoch 560/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.2459 - acc: 0.9207 - val_loss: 0.3995 - val_acc: 0.8775\n",
      "Epoch 561/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.2539 - acc: 0.9119 - val_loss: 0.4031 - val_acc: 0.8788\n",
      "Epoch 562/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.2544 - acc: 0.9186 - val_loss: 0.4563 - val_acc: 0.8549\n",
      "Epoch 563/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.2646 - acc: 0.9083 - val_loss: 0.4016 - val_acc: 0.8763\n",
      "Epoch 564/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.2401 - acc: 0.9170 - val_loss: 0.3938 - val_acc: 0.8751\n",
      "Epoch 565/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.2388 - acc: 0.9213 - val_loss: 0.4105 - val_acc: 0.8732\n",
      "Epoch 566/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.2553 - acc: 0.9095 - val_loss: 0.4169 - val_acc: 0.8714\n",
      "Epoch 567/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.2507 - acc: 0.9189 - val_loss: 0.3957 - val_acc: 0.8824\n",
      "Epoch 568/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.2484 - acc: 0.9128 - val_loss: 0.4308 - val_acc: 0.8628\n",
      "Epoch 569/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.2368 - acc: 0.9113 - val_loss: 0.4162 - val_acc: 0.8775\n",
      "Epoch 570/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.2614 - acc: 0.9005 - val_loss: 0.3857 - val_acc: 0.8836\n",
      "Epoch 571/1000\n",
      "3315/3315 [==============================] - 2s 546us/step - loss: 0.2641 - acc: 0.9119 - val_loss: 0.4041 - val_acc: 0.8824\n",
      "Epoch 572/1000\n",
      "3315/3315 [==============================] - 2s 524us/step - loss: 0.2360 - acc: 0.9213 - val_loss: 0.4422 - val_acc: 0.8610\n",
      "Epoch 573/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.2460 - acc: 0.9146 - val_loss: 0.4180 - val_acc: 0.8708\n",
      "Epoch 574/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.2497 - acc: 0.9119 - val_loss: 0.4134 - val_acc: 0.8775\n",
      "Epoch 575/1000\n",
      "3315/3315 [==============================] - 2s 530us/step - loss: 0.2506 - acc: 0.9176 - val_loss: 0.4151 - val_acc: 0.8690\n",
      "Epoch 576/1000\n",
      "3315/3315 [==============================] - 2s 529us/step - loss: 0.2208 - acc: 0.9261 - val_loss: 0.3873 - val_acc: 0.8867\n",
      "Epoch 577/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.2437 - acc: 0.9179 - val_loss: 0.4062 - val_acc: 0.8788\n",
      "Epoch 578/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.2624 - acc: 0.9083 - val_loss: 0.4254 - val_acc: 0.8751\n",
      "Epoch 579/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.2508 - acc: 0.9125 - val_loss: 0.4098 - val_acc: 0.8818\n",
      "Epoch 580/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 0.2489 - acc: 0.9122 - val_loss: 0.4117 - val_acc: 0.8757\n",
      "Epoch 581/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.2573 - acc: 0.9125 - val_loss: 0.3926 - val_acc: 0.8800\n",
      "Epoch 582/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 0.2406 - acc: 0.9110 - val_loss: 0.3953 - val_acc: 0.8757\n",
      "Epoch 583/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.2484 - acc: 0.9155 - val_loss: 0.3925 - val_acc: 0.8861\n",
      "Epoch 584/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.2347 - acc: 0.9192 - val_loss: 0.4077 - val_acc: 0.8794\n",
      "Epoch 585/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 0.2475 - acc: 0.9092 - val_loss: 0.3980 - val_acc: 0.8873\n",
      "Epoch 586/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.2338 - acc: 0.9195 - val_loss: 0.4069 - val_acc: 0.8788\n",
      "Epoch 587/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.2364 - acc: 0.9152 - val_loss: 0.4389 - val_acc: 0.8671\n",
      "Epoch 588/1000\n",
      "3315/3315 [==============================] - 2s 524us/step - loss: 0.2418 - acc: 0.9161 - val_loss: 0.3882 - val_acc: 0.8836\n",
      "Epoch 589/1000\n",
      "3315/3315 [==============================] - 2s 514us/step - loss: 0.2420 - acc: 0.9137 - val_loss: 0.4159 - val_acc: 0.8781\n",
      "Epoch 590/1000\n",
      "3315/3315 [==============================] - 2s 525us/step - loss: 0.2297 - acc: 0.9264 - val_loss: 0.3923 - val_acc: 0.8910\n",
      "Epoch 591/1000\n",
      "3315/3315 [==============================] - 2s 522us/step - loss: 0.2333 - acc: 0.9234 - val_loss: 0.3726 - val_acc: 0.8879\n",
      "Epoch 592/1000\n",
      "3315/3315 [==============================] - 2s 519us/step - loss: 0.2359 - acc: 0.9195 - val_loss: 0.3947 - val_acc: 0.8849\n",
      "Epoch 593/1000\n",
      "3315/3315 [==============================] - 2s 522us/step - loss: 0.2188 - acc: 0.9288 - val_loss: 0.4121 - val_acc: 0.8763\n",
      "Epoch 594/1000\n",
      "3315/3315 [==============================] - 2s 524us/step - loss: 0.2450 - acc: 0.9131 - val_loss: 0.3956 - val_acc: 0.8794\n",
      "Epoch 595/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.2407 - acc: 0.9219 - val_loss: 0.3902 - val_acc: 0.8855\n",
      "Epoch 596/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 0.2308 - acc: 0.9179 - val_loss: 0.4065 - val_acc: 0.8665\n",
      "Epoch 597/1000\n",
      "3315/3315 [==============================] - 2s 528us/step - loss: 0.2333 - acc: 0.9237 - val_loss: 0.3903 - val_acc: 0.8794\n",
      "Epoch 598/1000\n",
      "3315/3315 [==============================] - 2s 530us/step - loss: 0.2494 - acc: 0.9173 - val_loss: 0.3940 - val_acc: 0.8843\n",
      "Epoch 599/1000\n",
      "3315/3315 [==============================] - 2s 525us/step - loss: 0.2359 - acc: 0.9170 - val_loss: 0.3918 - val_acc: 0.8849\n",
      "Epoch 600/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.2354 - acc: 0.9195 - val_loss: 0.3976 - val_acc: 0.8800\n",
      "Epoch 601/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.2243 - acc: 0.9179 - val_loss: 0.4275 - val_acc: 0.8806\n",
      "Epoch 602/1000\n",
      "3315/3315 [==============================] - 2s 547us/step - loss: 0.2364 - acc: 0.9140 - val_loss: 0.3861 - val_acc: 0.8836\n",
      "Epoch 603/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.2311 - acc: 0.9231 - val_loss: 0.3884 - val_acc: 0.8812\n",
      "Epoch 604/1000\n",
      "3315/3315 [==============================] - 2s 552us/step - loss: 0.2443 - acc: 0.9207 - val_loss: 0.3812 - val_acc: 0.8928\n",
      "Epoch 605/1000\n",
      "3315/3315 [==============================] - 2s 578us/step - loss: 0.2146 - acc: 0.9261 - val_loss: 0.4016 - val_acc: 0.8861\n",
      "Epoch 606/1000\n",
      "3315/3315 [==============================] - 2s 585us/step - loss: 0.2379 - acc: 0.9131 - val_loss: 0.3876 - val_acc: 0.8910\n",
      "Epoch 607/1000\n",
      "3315/3315 [==============================] - 2s 569us/step - loss: 0.2275 - acc: 0.9198 - val_loss: 0.4001 - val_acc: 0.8849\n",
      "Epoch 608/1000\n",
      "3315/3315 [==============================] - 2s 529us/step - loss: 0.2288 - acc: 0.9240 - val_loss: 0.3865 - val_acc: 0.8867\n",
      "Epoch 609/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.2252 - acc: 0.9246 - val_loss: 0.4009 - val_acc: 0.8824\n",
      "Epoch 610/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.2292 - acc: 0.9213 - val_loss: 0.4170 - val_acc: 0.8714\n",
      "Epoch 611/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.2359 - acc: 0.9201 - val_loss: 0.3880 - val_acc: 0.8922\n",
      "Epoch 612/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.2173 - acc: 0.9288 - val_loss: 0.4145 - val_acc: 0.8671\n",
      "Epoch 613/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.2238 - acc: 0.9249 - val_loss: 0.4133 - val_acc: 0.8781\n",
      "Epoch 614/1000\n",
      "3315/3315 [==============================] - 2s 550us/step - loss: 0.2201 - acc: 0.9276 - val_loss: 0.4197 - val_acc: 0.8757\n",
      "Epoch 615/1000\n",
      "3315/3315 [==============================] - 2s 547us/step - loss: 0.2367 - acc: 0.9170 - val_loss: 0.3983 - val_acc: 0.8836\n",
      "Epoch 616/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.2247 - acc: 0.9261 - val_loss: 0.3936 - val_acc: 0.8898\n",
      "Epoch 617/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.2177 - acc: 0.9285 - val_loss: 0.3847 - val_acc: 0.8855\n",
      "Epoch 618/1000\n",
      "3315/3315 [==============================] - 2s 548us/step - loss: 0.2259 - acc: 0.9222 - val_loss: 0.3786 - val_acc: 0.8922\n",
      "Epoch 619/1000\n",
      "3315/3315 [==============================] - 2s 546us/step - loss: 0.2125 - acc: 0.9231 - val_loss: 0.4137 - val_acc: 0.8800\n",
      "Epoch 620/1000\n",
      "3315/3315 [==============================] - 2s 546us/step - loss: 0.2278 - acc: 0.9222 - val_loss: 0.3833 - val_acc: 0.8916\n",
      "Epoch 621/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.2269 - acc: 0.9243 - val_loss: 0.3904 - val_acc: 0.8928\n",
      "Epoch 622/1000\n",
      "3315/3315 [==============================] - 2s 557us/step - loss: 0.2180 - acc: 0.9228 - val_loss: 0.3782 - val_acc: 0.8941\n",
      "Epoch 623/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.2249 - acc: 0.9279 - val_loss: 0.4005 - val_acc: 0.8861\n",
      "Epoch 624/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.2242 - acc: 0.9255 - val_loss: 0.3985 - val_acc: 0.8812\n",
      "Epoch 625/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.2047 - acc: 0.9291 - val_loss: 0.3817 - val_acc: 0.8885\n",
      "Epoch 626/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.2264 - acc: 0.9255 - val_loss: 0.3882 - val_acc: 0.8928\n",
      "Epoch 627/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.2117 - acc: 0.9267 - val_loss: 0.3702 - val_acc: 0.8959\n",
      "Epoch 628/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.2245 - acc: 0.9267 - val_loss: 0.4066 - val_acc: 0.8879\n",
      "Epoch 629/1000\n",
      "3315/3315 [==============================] - 2s 557us/step - loss: 0.2093 - acc: 0.9342 - val_loss: 0.4003 - val_acc: 0.8861\n",
      "Epoch 630/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.2187 - acc: 0.9216 - val_loss: 0.3886 - val_acc: 0.8879\n",
      "Epoch 631/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.2230 - acc: 0.9261 - val_loss: 0.3958 - val_acc: 0.8824\n",
      "Epoch 632/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.2049 - acc: 0.9297 - val_loss: 0.3975 - val_acc: 0.8934\n",
      "Epoch 633/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.2303 - acc: 0.9222 - val_loss: 0.3680 - val_acc: 0.8867\n",
      "Epoch 634/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.2204 - acc: 0.9270 - val_loss: 0.3897 - val_acc: 0.8867\n",
      "Epoch 635/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.2154 - acc: 0.9288 - val_loss: 0.3982 - val_acc: 0.8953\n",
      "Epoch 636/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.2015 - acc: 0.9315 - val_loss: 0.3838 - val_acc: 0.8934\n",
      "Epoch 637/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.2214 - acc: 0.9291 - val_loss: 0.3882 - val_acc: 0.8892\n",
      "Epoch 638/1000\n",
      "3315/3315 [==============================] - 2s 553us/step - loss: 0.2052 - acc: 0.9312 - val_loss: 0.3644 - val_acc: 0.8990\n",
      "Epoch 639/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.2095 - acc: 0.9267 - val_loss: 0.3722 - val_acc: 0.8971\n",
      "Epoch 640/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 0.2278 - acc: 0.9228 - val_loss: 0.3822 - val_acc: 0.8934\n",
      "Epoch 641/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.2018 - acc: 0.9330 - val_loss: 0.3830 - val_acc: 0.8867\n",
      "Epoch 642/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.2270 - acc: 0.9285 - val_loss: 0.3654 - val_acc: 0.8983\n",
      "Epoch 643/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.2187 - acc: 0.9264 - val_loss: 0.3771 - val_acc: 0.8892\n",
      "Epoch 644/1000\n",
      "3315/3315 [==============================] - 2s 549us/step - loss: 0.2173 - acc: 0.9249 - val_loss: 0.3770 - val_acc: 0.9032\n",
      "Epoch 645/1000\n",
      "3315/3315 [==============================] - 2s 551us/step - loss: 0.2131 - acc: 0.9258 - val_loss: 0.3863 - val_acc: 0.8855\n",
      "Epoch 646/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.2063 - acc: 0.9348 - val_loss: 0.3849 - val_acc: 0.8873\n",
      "Epoch 647/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.2114 - acc: 0.9306 - val_loss: 0.3775 - val_acc: 0.8873\n",
      "Epoch 648/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.2108 - acc: 0.9264 - val_loss: 0.3712 - val_acc: 0.8941\n",
      "Epoch 649/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.2113 - acc: 0.9240 - val_loss: 0.4105 - val_acc: 0.8812\n",
      "Epoch 650/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.2036 - acc: 0.9297 - val_loss: 0.3814 - val_acc: 0.8971\n",
      "Epoch 651/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.2058 - acc: 0.9321 - val_loss: 0.4031 - val_acc: 0.8812\n",
      "Epoch 652/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.2077 - acc: 0.9282 - val_loss: 0.3835 - val_acc: 0.8916\n",
      "Epoch 653/1000\n",
      "3315/3315 [==============================] - 2s 570us/step - loss: 0.2070 - acc: 0.9333 - val_loss: 0.3791 - val_acc: 0.8916\n",
      "Epoch 654/1000\n",
      "3315/3315 [==============================] - 2s 572us/step - loss: 0.2103 - acc: 0.9273 - val_loss: 0.3903 - val_acc: 0.8824\n",
      "Epoch 655/1000\n",
      "3315/3315 [==============================] - 2s 570us/step - loss: 0.2135 - acc: 0.9252 - val_loss: 0.3742 - val_acc: 0.8916\n",
      "Epoch 656/1000\n",
      "3315/3315 [==============================] - 2s 573us/step - loss: 0.2165 - acc: 0.9249 - val_loss: 0.3801 - val_acc: 0.8959\n",
      "Epoch 657/1000\n",
      "3315/3315 [==============================] - 2s 566us/step - loss: 0.2062 - acc: 0.9291 - val_loss: 0.3850 - val_acc: 0.8934\n",
      "Epoch 658/1000\n",
      "3315/3315 [==============================] - 2s 554us/step - loss: 0.2240 - acc: 0.9201 - val_loss: 0.3994 - val_acc: 0.8836\n",
      "Epoch 659/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.2066 - acc: 0.9318 - val_loss: 0.3868 - val_acc: 0.8806\n",
      "Epoch 660/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.2102 - acc: 0.9234 - val_loss: 0.3887 - val_acc: 0.8910\n",
      "Epoch 661/1000\n",
      "3315/3315 [==============================] - 2s 550us/step - loss: 0.1993 - acc: 0.9297 - val_loss: 0.3698 - val_acc: 0.9008\n",
      "Epoch 662/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.2094 - acc: 0.9303 - val_loss: 0.3847 - val_acc: 0.8922\n",
      "Epoch 663/1000\n",
      "3315/3315 [==============================] - 2s 548us/step - loss: 0.2031 - acc: 0.9321 - val_loss: 0.4033 - val_acc: 0.8824\n",
      "Epoch 664/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.2054 - acc: 0.9321 - val_loss: 0.3577 - val_acc: 0.8941\n",
      "Epoch 665/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.1968 - acc: 0.9345 - val_loss: 0.3846 - val_acc: 0.8806\n",
      "Epoch 666/1000\n",
      "3315/3315 [==============================] - 2s 549us/step - loss: 0.1987 - acc: 0.9291 - val_loss: 0.3751 - val_acc: 0.8983\n",
      "Epoch 667/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.2096 - acc: 0.9282 - val_loss: 0.3595 - val_acc: 0.8990\n",
      "Epoch 668/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.1981 - acc: 0.9360 - val_loss: 0.3740 - val_acc: 0.8959\n",
      "Epoch 669/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.2227 - acc: 0.9255 - val_loss: 0.3757 - val_acc: 0.8928\n",
      "Epoch 670/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.2092 - acc: 0.9303 - val_loss: 0.3850 - val_acc: 0.8941\n",
      "Epoch 671/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.1933 - acc: 0.9348 - val_loss: 0.3765 - val_acc: 0.8892\n",
      "Epoch 672/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.1897 - acc: 0.9382 - val_loss: 0.3761 - val_acc: 0.8898\n",
      "Epoch 673/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.1888 - acc: 0.9351 - val_loss: 0.3862 - val_acc: 0.8934\n",
      "Epoch 674/1000\n",
      "3315/3315 [==============================] - 2s 551us/step - loss: 0.2010 - acc: 0.9354 - val_loss: 0.3667 - val_acc: 0.8934\n",
      "Epoch 675/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.2037 - acc: 0.9291 - val_loss: 0.3755 - val_acc: 0.8959\n",
      "Epoch 676/1000\n",
      "3315/3315 [==============================] - 2s 546us/step - loss: 0.1902 - acc: 0.9357 - val_loss: 0.3969 - val_acc: 0.8855\n",
      "Epoch 677/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.1946 - acc: 0.9351 - val_loss: 0.3695 - val_acc: 0.9039\n",
      "Epoch 678/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.1843 - acc: 0.9379 - val_loss: 0.3783 - val_acc: 0.8873\n",
      "Epoch 679/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.1951 - acc: 0.9321 - val_loss: 0.3602 - val_acc: 0.8971\n",
      "Epoch 680/1000\n",
      "3315/3315 [==============================] - 2s 547us/step - loss: 0.1833 - acc: 0.9382 - val_loss: 0.3951 - val_acc: 0.8855\n",
      "Epoch 681/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.1913 - acc: 0.9282 - val_loss: 0.3495 - val_acc: 0.9008\n",
      "Epoch 682/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.1977 - acc: 0.9339 - val_loss: 0.3640 - val_acc: 0.9020\n",
      "Epoch 683/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.2010 - acc: 0.9300 - val_loss: 0.3707 - val_acc: 0.8879\n",
      "Epoch 684/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.1946 - acc: 0.9360 - val_loss: 0.3894 - val_acc: 0.8879\n",
      "Epoch 685/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.1985 - acc: 0.9342 - val_loss: 0.3784 - val_acc: 0.8941\n",
      "Epoch 686/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.1926 - acc: 0.9406 - val_loss: 0.3637 - val_acc: 0.9002\n",
      "Epoch 687/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.1867 - acc: 0.9406 - val_loss: 0.3780 - val_acc: 0.8892\n",
      "Epoch 688/1000\n",
      "3315/3315 [==============================] - 2s 547us/step - loss: 0.1947 - acc: 0.9339 - val_loss: 0.3660 - val_acc: 0.9032\n",
      "Epoch 689/1000\n",
      "3315/3315 [==============================] - 2s 554us/step - loss: 0.1908 - acc: 0.9400 - val_loss: 0.3582 - val_acc: 0.9014\n",
      "Epoch 690/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.2047 - acc: 0.9306 - val_loss: 0.3755 - val_acc: 0.8928\n",
      "Epoch 691/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.1887 - acc: 0.9367 - val_loss: 0.3755 - val_acc: 0.8947\n",
      "Epoch 692/1000\n",
      "3315/3315 [==============================] - 2s 548us/step - loss: 0.1833 - acc: 0.9421 - val_loss: 0.3695 - val_acc: 0.8928\n",
      "Epoch 693/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.1869 - acc: 0.9397 - val_loss: 0.3676 - val_acc: 0.9020\n",
      "Epoch 694/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.1924 - acc: 0.9363 - val_loss: 0.3749 - val_acc: 0.8971\n",
      "Epoch 695/1000\n",
      "3315/3315 [==============================] - 2s 546us/step - loss: 0.1945 - acc: 0.9354 - val_loss: 0.3838 - val_acc: 0.8855\n",
      "Epoch 696/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.1976 - acc: 0.9354 - val_loss: 0.4119 - val_acc: 0.8714\n",
      "Epoch 697/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.1963 - acc: 0.9327 - val_loss: 0.3612 - val_acc: 0.8885\n",
      "Epoch 698/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.1901 - acc: 0.9367 - val_loss: 0.3630 - val_acc: 0.8977\n",
      "Epoch 699/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.1882 - acc: 0.9318 - val_loss: 0.3685 - val_acc: 0.8934\n",
      "Epoch 700/1000\n",
      "3315/3315 [==============================] - 2s 546us/step - loss: 0.1828 - acc: 0.9406 - val_loss: 0.3494 - val_acc: 0.9026\n",
      "Epoch 701/1000\n",
      "3315/3315 [==============================] - 2s 547us/step - loss: 0.1830 - acc: 0.9397 - val_loss: 0.3673 - val_acc: 0.8934\n",
      "Epoch 702/1000\n",
      "3315/3315 [==============================] - 2s 551us/step - loss: 0.1826 - acc: 0.9397 - val_loss: 0.3597 - val_acc: 0.8892\n",
      "Epoch 703/1000\n",
      "3315/3315 [==============================] - 2s 550us/step - loss: 0.1823 - acc: 0.9412 - val_loss: 0.3571 - val_acc: 0.8922\n",
      "Epoch 704/1000\n",
      "3315/3315 [==============================] - 2s 550us/step - loss: 0.1889 - acc: 0.9333 - val_loss: 0.3726 - val_acc: 0.8983\n",
      "Epoch 705/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.1928 - acc: 0.9373 - val_loss: 0.3782 - val_acc: 0.8934\n",
      "Epoch 706/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.1978 - acc: 0.9336 - val_loss: 0.3582 - val_acc: 0.9020\n",
      "Epoch 707/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.2057 - acc: 0.9300 - val_loss: 0.3879 - val_acc: 0.8904\n",
      "Epoch 708/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.1861 - acc: 0.9421 - val_loss: 0.3811 - val_acc: 0.8947\n",
      "Epoch 709/1000\n",
      "3315/3315 [==============================] - 2s 527us/step - loss: 0.1991 - acc: 0.9318 - val_loss: 0.3703 - val_acc: 0.8928\n",
      "Epoch 710/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.1860 - acc: 0.9397 - val_loss: 0.3720 - val_acc: 0.8947\n",
      "Epoch 711/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.1824 - acc: 0.9336 - val_loss: 0.3760 - val_acc: 0.8916\n",
      "Epoch 712/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.1960 - acc: 0.9363 - val_loss: 0.3748 - val_acc: 0.8971\n",
      "Epoch 713/1000\n",
      "3315/3315 [==============================] - 2s 522us/step - loss: 0.1843 - acc: 0.9376 - val_loss: 0.3861 - val_acc: 0.8843\n",
      "Epoch 714/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.1669 - acc: 0.9439 - val_loss: 0.3742 - val_acc: 0.8941\n",
      "Epoch 715/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.1973 - acc: 0.9306 - val_loss: 0.3732 - val_acc: 0.8983\n",
      "Epoch 716/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.1757 - acc: 0.9376 - val_loss: 0.3765 - val_acc: 0.8934\n",
      "Epoch 717/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.1829 - acc: 0.9430 - val_loss: 0.3669 - val_acc: 0.9002\n",
      "Epoch 718/1000\n",
      "3315/3315 [==============================] - 2s 529us/step - loss: 0.1952 - acc: 0.9291 - val_loss: 0.3595 - val_acc: 0.9002\n",
      "Epoch 719/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.1837 - acc: 0.9342 - val_loss: 0.3561 - val_acc: 0.9088\n",
      "Epoch 720/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.1868 - acc: 0.9379 - val_loss: 0.3675 - val_acc: 0.8990\n",
      "Epoch 721/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.1792 - acc: 0.9418 - val_loss: 0.3593 - val_acc: 0.9014\n",
      "Epoch 722/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.1924 - acc: 0.9397 - val_loss: 0.3596 - val_acc: 0.9020\n",
      "Epoch 723/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 0.1824 - acc: 0.9415 - val_loss: 0.3710 - val_acc: 0.8996\n",
      "Epoch 724/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.1773 - acc: 0.9439 - val_loss: 0.3833 - val_acc: 0.8885\n",
      "Epoch 725/1000\n",
      "3315/3315 [==============================] - 2s 524us/step - loss: 0.1713 - acc: 0.9400 - val_loss: 0.3720 - val_acc: 0.8941\n",
      "Epoch 726/1000\n",
      "3315/3315 [==============================] - 2s 514us/step - loss: 0.1721 - acc: 0.9376 - val_loss: 0.3605 - val_acc: 0.8971\n",
      "Epoch 727/1000\n",
      "3315/3315 [==============================] - 2s 519us/step - loss: 0.1792 - acc: 0.9370 - val_loss: 0.3543 - val_acc: 0.8996\n",
      "Epoch 728/1000\n",
      "3315/3315 [==============================] - 2s 524us/step - loss: 0.1808 - acc: 0.9394 - val_loss: 0.3704 - val_acc: 0.8959\n",
      "Epoch 729/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.1707 - acc: 0.9421 - val_loss: 0.3757 - val_acc: 0.8916\n",
      "Epoch 730/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.1834 - acc: 0.9367 - val_loss: 0.3551 - val_acc: 0.8996\n",
      "Epoch 731/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.1879 - acc: 0.9336 - val_loss: 0.3589 - val_acc: 0.8959\n",
      "Epoch 732/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.1853 - acc: 0.9318 - val_loss: 0.3644 - val_acc: 0.9026\n",
      "Epoch 733/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.1873 - acc: 0.9379 - val_loss: 0.3692 - val_acc: 0.8910\n",
      "Epoch 734/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.1823 - acc: 0.9385 - val_loss: 0.3701 - val_acc: 0.9039\n",
      "Epoch 735/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.1623 - acc: 0.9463 - val_loss: 0.3588 - val_acc: 0.9008\n",
      "Epoch 736/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.1956 - acc: 0.9339 - val_loss: 0.3678 - val_acc: 0.8941\n",
      "Epoch 737/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.1775 - acc: 0.9421 - val_loss: 0.3532 - val_acc: 0.9051\n",
      "Epoch 738/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.1732 - acc: 0.9409 - val_loss: 0.3529 - val_acc: 0.9045\n",
      "Epoch 739/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.1710 - acc: 0.9427 - val_loss: 0.3540 - val_acc: 0.9057\n",
      "Epoch 740/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.1882 - acc: 0.9318 - val_loss: 0.3518 - val_acc: 0.9057\n",
      "Epoch 741/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.1805 - acc: 0.9397 - val_loss: 0.3680 - val_acc: 0.8971\n",
      "Epoch 742/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.1831 - acc: 0.9379 - val_loss: 0.3881 - val_acc: 0.8873\n",
      "Epoch 743/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.1660 - acc: 0.9403 - val_loss: 0.3698 - val_acc: 0.9045\n",
      "Epoch 744/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.1648 - acc: 0.9436 - val_loss: 0.3714 - val_acc: 0.8947\n",
      "Epoch 745/1000\n",
      "3315/3315 [==============================] - 2s 548us/step - loss: 0.1745 - acc: 0.9418 - val_loss: 0.3677 - val_acc: 0.8971\n",
      "Epoch 746/1000\n",
      "3315/3315 [==============================] - 2s 552us/step - loss: 0.1781 - acc: 0.9418 - val_loss: 0.3603 - val_acc: 0.8916\n",
      "Epoch 747/1000\n",
      "3315/3315 [==============================] - 2s 552us/step - loss: 0.1700 - acc: 0.9430 - val_loss: 0.3686 - val_acc: 0.8922\n",
      "Epoch 748/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.1849 - acc: 0.9403 - val_loss: 0.3870 - val_acc: 0.8928\n",
      "Epoch 749/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.1689 - acc: 0.9466 - val_loss: 0.3778 - val_acc: 0.8996\n",
      "Epoch 750/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.1757 - acc: 0.9424 - val_loss: 0.3502 - val_acc: 0.9032\n",
      "Epoch 751/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.1894 - acc: 0.9397 - val_loss: 0.3835 - val_acc: 0.8861\n",
      "Epoch 752/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.1676 - acc: 0.9421 - val_loss: 0.3584 - val_acc: 0.8996\n",
      "Epoch 753/1000\n",
      "3315/3315 [==============================] - 2s 530us/step - loss: 0.1803 - acc: 0.9345 - val_loss: 0.3646 - val_acc: 0.8941\n",
      "Epoch 754/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.1616 - acc: 0.9433 - val_loss: 0.3671 - val_acc: 0.9081\n",
      "Epoch 755/1000\n",
      "3315/3315 [==============================] - 2s 555us/step - loss: 0.1714 - acc: 0.9439 - val_loss: 0.3688 - val_acc: 0.9020\n",
      "Epoch 756/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 0.1638 - acc: 0.9484 - val_loss: 0.3717 - val_acc: 0.8977\n",
      "Epoch 757/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.1797 - acc: 0.9379 - val_loss: 0.3646 - val_acc: 0.9069\n",
      "Epoch 758/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.1716 - acc: 0.9442 - val_loss: 0.3742 - val_acc: 0.8916\n",
      "Epoch 759/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.1748 - acc: 0.9363 - val_loss: 0.3676 - val_acc: 0.9020\n",
      "Epoch 760/1000\n",
      "3315/3315 [==============================] - 2s 547us/step - loss: 0.1785 - acc: 0.9373 - val_loss: 0.3548 - val_acc: 0.9063\n",
      "Epoch 761/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.1634 - acc: 0.9466 - val_loss: 0.3794 - val_acc: 0.8971\n",
      "Epoch 762/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.1748 - acc: 0.9424 - val_loss: 0.3538 - val_acc: 0.9045\n",
      "Epoch 763/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.1678 - acc: 0.9445 - val_loss: 0.3675 - val_acc: 0.8983\n",
      "Epoch 764/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.1680 - acc: 0.9427 - val_loss: 0.3592 - val_acc: 0.9057\n",
      "Epoch 765/1000\n",
      "3315/3315 [==============================] - 2s 522us/step - loss: 0.1748 - acc: 0.9397 - val_loss: 0.3630 - val_acc: 0.8953\n",
      "Epoch 766/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.1807 - acc: 0.9376 - val_loss: 0.3574 - val_acc: 0.9045\n",
      "Epoch 767/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 0.1690 - acc: 0.9490 - val_loss: 0.3589 - val_acc: 0.9032\n",
      "Epoch 768/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.1699 - acc: 0.9436 - val_loss: 0.3632 - val_acc: 0.8959\n",
      "Epoch 769/1000\n",
      "3315/3315 [==============================] - 2s 526us/step - loss: 0.1689 - acc: 0.9412 - val_loss: 0.3613 - val_acc: 0.9002\n",
      "Epoch 770/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.1623 - acc: 0.9478 - val_loss: 0.3515 - val_acc: 0.9100\n",
      "Epoch 771/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.1639 - acc: 0.9385 - val_loss: 0.3677 - val_acc: 0.8971\n",
      "Epoch 772/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.1669 - acc: 0.9502 - val_loss: 0.3744 - val_acc: 0.8959\n",
      "Epoch 773/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 0.1790 - acc: 0.9403 - val_loss: 0.3670 - val_acc: 0.8965\n",
      "Epoch 774/1000\n",
      "3315/3315 [==============================] - 2s 526us/step - loss: 0.1659 - acc: 0.9454 - val_loss: 0.3411 - val_acc: 0.9045\n",
      "Epoch 775/1000\n",
      "3315/3315 [==============================] - 2s 522us/step - loss: 0.1836 - acc: 0.9354 - val_loss: 0.3903 - val_acc: 0.8953\n",
      "Epoch 776/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.1677 - acc: 0.9457 - val_loss: 0.3506 - val_acc: 0.9002\n",
      "Epoch 777/1000\n",
      "3315/3315 [==============================] - 2s 578us/step - loss: 0.1666 - acc: 0.9454 - val_loss: 0.4044 - val_acc: 0.8861\n",
      "Epoch 778/1000\n",
      "3315/3315 [==============================] - 2s 568us/step - loss: 0.1754 - acc: 0.9379 - val_loss: 0.3590 - val_acc: 0.9039\n",
      "Epoch 779/1000\n",
      "3315/3315 [==============================] - 2s 561us/step - loss: 0.1720 - acc: 0.9400 - val_loss: 0.3555 - val_acc: 0.9014\n",
      "Epoch 780/1000\n",
      "3315/3315 [==============================] - 2s 527us/step - loss: 0.1514 - acc: 0.9481 - val_loss: 0.3541 - val_acc: 0.8990\n",
      "Epoch 781/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.1686 - acc: 0.9463 - val_loss: 0.3750 - val_acc: 0.8977\n",
      "Epoch 782/1000\n",
      "3315/3315 [==============================] - 2s 528us/step - loss: 0.1609 - acc: 0.9439 - val_loss: 0.3475 - val_acc: 0.9008\n",
      "Epoch 783/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.1577 - acc: 0.9433 - val_loss: 0.3796 - val_acc: 0.9026\n",
      "Epoch 784/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.1614 - acc: 0.9418 - val_loss: 0.3675 - val_acc: 0.8959\n",
      "Epoch 785/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.1763 - acc: 0.9427 - val_loss: 0.3547 - val_acc: 0.8996\n",
      "Epoch 786/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.1627 - acc: 0.9448 - val_loss: 0.3525 - val_acc: 0.8983\n",
      "Epoch 787/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.1557 - acc: 0.9451 - val_loss: 0.3500 - val_acc: 0.9039\n",
      "Epoch 788/1000\n",
      "3315/3315 [==============================] - 2s 547us/step - loss: 0.1625 - acc: 0.9439 - val_loss: 0.3755 - val_acc: 0.9002\n",
      "Epoch 789/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.1799 - acc: 0.9367 - val_loss: 0.3773 - val_acc: 0.8941\n",
      "Epoch 790/1000\n",
      "3315/3315 [==============================] - 2s 526us/step - loss: 0.1638 - acc: 0.9457 - val_loss: 0.3650 - val_acc: 0.9008\n",
      "Epoch 791/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.1602 - acc: 0.9457 - val_loss: 0.3530 - val_acc: 0.9014\n",
      "Epoch 792/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 0.1623 - acc: 0.9478 - val_loss: 0.3702 - val_acc: 0.8971\n",
      "Epoch 793/1000\n",
      "3315/3315 [==============================] - 2s 526us/step - loss: 0.1539 - acc: 0.9430 - val_loss: 0.3524 - val_acc: 0.8934\n",
      "Epoch 794/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.1626 - acc: 0.9433 - val_loss: 0.3514 - val_acc: 0.8977\n",
      "Epoch 795/1000\n",
      "3315/3315 [==============================] - 2s 530us/step - loss: 0.1589 - acc: 0.9421 - val_loss: 0.3831 - val_acc: 0.8904\n",
      "Epoch 796/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.1485 - acc: 0.9526 - val_loss: 0.3968 - val_acc: 0.8885\n",
      "Epoch 797/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.1615 - acc: 0.9430 - val_loss: 0.3546 - val_acc: 0.9002\n",
      "Epoch 798/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 0.1622 - acc: 0.9448 - val_loss: 0.3616 - val_acc: 0.8990\n",
      "Epoch 799/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 0.1555 - acc: 0.9493 - val_loss: 0.3676 - val_acc: 0.9032\n",
      "Epoch 800/1000\n",
      "3315/3315 [==============================] - 2s 528us/step - loss: 0.1493 - acc: 0.9478 - val_loss: 0.3770 - val_acc: 0.8928\n",
      "Epoch 801/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.1676 - acc: 0.9415 - val_loss: 0.3543 - val_acc: 0.9008\n",
      "Epoch 802/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.1467 - acc: 0.9514 - val_loss: 0.3496 - val_acc: 0.8990\n",
      "Epoch 803/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.1592 - acc: 0.9463 - val_loss: 0.3509 - val_acc: 0.9063\n",
      "Epoch 804/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 0.1558 - acc: 0.9481 - val_loss: 0.3648 - val_acc: 0.9039\n",
      "Epoch 805/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.1572 - acc: 0.9511 - val_loss: 0.3705 - val_acc: 0.9075\n",
      "Epoch 806/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.1631 - acc: 0.9454 - val_loss: 0.3771 - val_acc: 0.9032\n",
      "Epoch 807/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.1425 - acc: 0.9505 - val_loss: 0.3565 - val_acc: 0.9020\n",
      "Epoch 808/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.1592 - acc: 0.9472 - val_loss: 0.3565 - val_acc: 0.9014\n",
      "Epoch 809/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.1533 - acc: 0.9454 - val_loss: 0.3445 - val_acc: 0.9094\n",
      "Epoch 810/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.1542 - acc: 0.9451 - val_loss: 0.3754 - val_acc: 0.9026\n",
      "Epoch 811/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.1655 - acc: 0.9478 - val_loss: 0.3705 - val_acc: 0.9088\n",
      "Epoch 812/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.1613 - acc: 0.9469 - val_loss: 0.3648 - val_acc: 0.9026\n",
      "Epoch 813/1000\n",
      "3315/3315 [==============================] - 2s 546us/step - loss: 0.1550 - acc: 0.9493 - val_loss: 0.3673 - val_acc: 0.9057\n",
      "Epoch 814/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.1511 - acc: 0.9523 - val_loss: 0.3651 - val_acc: 0.9051\n",
      "Epoch 815/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.1565 - acc: 0.9487 - val_loss: 0.3521 - val_acc: 0.9051\n",
      "Epoch 816/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.1513 - acc: 0.9511 - val_loss: 0.3472 - val_acc: 0.9063\n",
      "Epoch 817/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.1576 - acc: 0.9481 - val_loss: 0.3874 - val_acc: 0.8941\n",
      "Epoch 818/1000\n",
      "3315/3315 [==============================] - 2s 555us/step - loss: 0.1413 - acc: 0.9529 - val_loss: 0.3447 - val_acc: 0.9026\n",
      "Epoch 819/1000\n",
      "3315/3315 [==============================] - 2s 548us/step - loss: 0.1411 - acc: 0.9526 - val_loss: 0.3486 - val_acc: 0.9063\n",
      "Epoch 820/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.1642 - acc: 0.9454 - val_loss: 0.3516 - val_acc: 0.9008\n",
      "Epoch 821/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.1486 - acc: 0.9499 - val_loss: 0.3498 - val_acc: 0.9081\n",
      "Epoch 822/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.1468 - acc: 0.9520 - val_loss: 0.3525 - val_acc: 0.9026\n",
      "Epoch 823/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.1653 - acc: 0.9457 - val_loss: 0.3532 - val_acc: 0.9081\n",
      "Epoch 824/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.1533 - acc: 0.9502 - val_loss: 0.3691 - val_acc: 0.9002\n",
      "Epoch 825/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.1567 - acc: 0.9469 - val_loss: 0.3487 - val_acc: 0.9088\n",
      "Epoch 826/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.1618 - acc: 0.9451 - val_loss: 0.3655 - val_acc: 0.9032\n",
      "Epoch 827/1000\n",
      "3315/3315 [==============================] - 2s 573us/step - loss: 0.1500 - acc: 0.9511 - val_loss: 0.3605 - val_acc: 0.9039\n",
      "Epoch 828/1000\n",
      "3315/3315 [==============================] - 2s 576us/step - loss: 0.1384 - acc: 0.9475 - val_loss: 0.3538 - val_acc: 0.8977\n",
      "Epoch 829/1000\n",
      "3315/3315 [==============================] - 2s 578us/step - loss: 0.1632 - acc: 0.9457 - val_loss: 0.3452 - val_acc: 0.9051\n",
      "Epoch 830/1000\n",
      "3315/3315 [==============================] - 2s 575us/step - loss: 0.1548 - acc: 0.9481 - val_loss: 0.3571 - val_acc: 0.9118\n",
      "Epoch 831/1000\n",
      "3315/3315 [==============================] - 2s 580us/step - loss: 0.1447 - acc: 0.9535 - val_loss: 0.3434 - val_acc: 0.9155\n",
      "Epoch 832/1000\n",
      "3315/3315 [==============================] - 2s 553us/step - loss: 0.1547 - acc: 0.9505 - val_loss: 0.3627 - val_acc: 0.9032\n",
      "Epoch 833/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.1521 - acc: 0.9487 - val_loss: 0.3465 - val_acc: 0.9069\n",
      "Epoch 834/1000\n",
      "3315/3315 [==============================] - 2s 551us/step - loss: 0.1480 - acc: 0.9514 - val_loss: 0.3705 - val_acc: 0.9020\n",
      "Epoch 835/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.1555 - acc: 0.9517 - val_loss: 0.3586 - val_acc: 0.9020\n",
      "Epoch 836/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.1545 - acc: 0.9496 - val_loss: 0.3444 - val_acc: 0.9112\n",
      "Epoch 837/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.1442 - acc: 0.9517 - val_loss: 0.3399 - val_acc: 0.9118\n",
      "Epoch 838/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.1507 - acc: 0.9529 - val_loss: 0.3640 - val_acc: 0.9094\n",
      "Epoch 839/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.1440 - acc: 0.9548 - val_loss: 0.3699 - val_acc: 0.9032\n",
      "Epoch 840/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.1644 - acc: 0.9511 - val_loss: 0.3648 - val_acc: 0.9002\n",
      "Epoch 841/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 0.1487 - acc: 0.9526 - val_loss: 0.3596 - val_acc: 0.9039\n",
      "Epoch 842/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.1365 - acc: 0.9538 - val_loss: 0.3812 - val_acc: 0.8904\n",
      "Epoch 843/1000\n",
      "3315/3315 [==============================] - 2s 546us/step - loss: 0.1373 - acc: 0.9511 - val_loss: 0.3561 - val_acc: 0.8953\n",
      "Epoch 844/1000\n",
      "3315/3315 [==============================] - 2s 549us/step - loss: 0.1385 - acc: 0.9514 - val_loss: 0.3721 - val_acc: 0.9002\n",
      "Epoch 845/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.1435 - acc: 0.9505 - val_loss: 0.3527 - val_acc: 0.9026\n",
      "Epoch 846/1000\n",
      "3315/3315 [==============================] - 2s 527us/step - loss: 0.1529 - acc: 0.9499 - val_loss: 0.3569 - val_acc: 0.9088\n",
      "Epoch 847/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.1367 - acc: 0.9523 - val_loss: 0.3483 - val_acc: 0.9026\n",
      "Epoch 848/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.1405 - acc: 0.9502 - val_loss: 0.3269 - val_acc: 0.9118\n",
      "Epoch 849/1000\n",
      "3315/3315 [==============================] - 2s 530us/step - loss: 0.1352 - acc: 0.9532 - val_loss: 0.3787 - val_acc: 0.9020\n",
      "Epoch 850/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.1465 - acc: 0.9538 - val_loss: 0.3614 - val_acc: 0.9112\n",
      "Epoch 851/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.1514 - acc: 0.9490 - val_loss: 0.3481 - val_acc: 0.9112\n",
      "Epoch 852/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.1473 - acc: 0.9514 - val_loss: 0.3593 - val_acc: 0.9069\n",
      "Epoch 853/1000\n",
      "3315/3315 [==============================] - 2s 549us/step - loss: 0.1404 - acc: 0.9538 - val_loss: 0.3636 - val_acc: 0.9045\n",
      "Epoch 854/1000\n",
      "3315/3315 [==============================] - 2s 550us/step - loss: 0.1606 - acc: 0.9472 - val_loss: 0.3716 - val_acc: 0.9020\n",
      "Epoch 855/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.1370 - acc: 0.9502 - val_loss: 0.3433 - val_acc: 0.9069\n",
      "Epoch 856/1000\n",
      "3315/3315 [==============================] - 2s 554us/step - loss: 0.1467 - acc: 0.9517 - val_loss: 0.3428 - val_acc: 0.9088\n",
      "Epoch 857/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.1439 - acc: 0.9520 - val_loss: 0.3447 - val_acc: 0.9106\n",
      "Epoch 858/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.1528 - acc: 0.9463 - val_loss: 0.3575 - val_acc: 0.9051\n",
      "Epoch 859/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.1428 - acc: 0.9505 - val_loss: 0.3541 - val_acc: 0.9063\n",
      "Epoch 860/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.1458 - acc: 0.9502 - val_loss: 0.3395 - val_acc: 0.9112\n",
      "Epoch 861/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.1488 - acc: 0.9499 - val_loss: 0.3464 - val_acc: 0.9075\n",
      "Epoch 862/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.1377 - acc: 0.9554 - val_loss: 0.3364 - val_acc: 0.9124\n",
      "Epoch 863/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.1475 - acc: 0.9508 - val_loss: 0.3460 - val_acc: 0.9100\n",
      "Epoch 864/1000\n",
      "3315/3315 [==============================] - 2s 554us/step - loss: 0.1513 - acc: 0.9493 - val_loss: 0.3461 - val_acc: 0.9106\n",
      "Epoch 865/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.1407 - acc: 0.9529 - val_loss: 0.3541 - val_acc: 0.9057\n",
      "Epoch 866/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.1536 - acc: 0.9478 - val_loss: 0.3390 - val_acc: 0.9081\n",
      "Epoch 867/1000\n",
      "3315/3315 [==============================] - 2s 551us/step - loss: 0.1444 - acc: 0.9529 - val_loss: 0.3512 - val_acc: 0.9088\n",
      "Epoch 868/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.1385 - acc: 0.9526 - val_loss: 0.3614 - val_acc: 0.9045\n",
      "Epoch 869/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.1444 - acc: 0.9541 - val_loss: 0.3536 - val_acc: 0.9032\n",
      "Epoch 870/1000\n",
      "3315/3315 [==============================] - 2s 558us/step - loss: 0.1469 - acc: 0.9475 - val_loss: 0.3431 - val_acc: 0.9039\n",
      "Epoch 871/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.1439 - acc: 0.9508 - val_loss: 0.3570 - val_acc: 0.9057\n",
      "Epoch 872/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.1340 - acc: 0.9572 - val_loss: 0.3657 - val_acc: 0.9069\n",
      "Epoch 873/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.1491 - acc: 0.9529 - val_loss: 0.3402 - val_acc: 0.9100\n",
      "Epoch 874/1000\n",
      "3315/3315 [==============================] - 2s 549us/step - loss: 0.1373 - acc: 0.9511 - val_loss: 0.3537 - val_acc: 0.9069\n",
      "Epoch 875/1000\n",
      "3315/3315 [==============================] - 2s 550us/step - loss: 0.1220 - acc: 0.9596 - val_loss: 0.3480 - val_acc: 0.9069\n",
      "Epoch 876/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.1379 - acc: 0.9505 - val_loss: 0.3484 - val_acc: 0.9088\n",
      "Epoch 877/1000\n",
      "3315/3315 [==============================] - 2s 546us/step - loss: 0.1322 - acc: 0.9517 - val_loss: 0.3471 - val_acc: 0.9088\n",
      "Epoch 878/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.1541 - acc: 0.9466 - val_loss: 0.3458 - val_acc: 0.9045\n",
      "Epoch 879/1000\n",
      "3315/3315 [==============================] - 2s 552us/step - loss: 0.1447 - acc: 0.9541 - val_loss: 0.3430 - val_acc: 0.9106\n",
      "Epoch 880/1000\n",
      "3315/3315 [==============================] - 2s 549us/step - loss: 0.1417 - acc: 0.9520 - val_loss: 0.3672 - val_acc: 0.8996\n",
      "Epoch 881/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.1437 - acc: 0.9535 - val_loss: 0.3594 - val_acc: 0.9081\n",
      "Epoch 882/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.1517 - acc: 0.9481 - val_loss: 0.3639 - val_acc: 0.9094\n",
      "Epoch 883/1000\n",
      "3315/3315 [==============================] - 2s 548us/step - loss: 0.1386 - acc: 0.9505 - val_loss: 0.3503 - val_acc: 0.9063\n",
      "Epoch 884/1000\n",
      "3315/3315 [==============================] - 2s 557us/step - loss: 0.1246 - acc: 0.9584 - val_loss: 0.3503 - val_acc: 0.9045\n",
      "Epoch 885/1000\n",
      "3315/3315 [==============================] - 2s 553us/step - loss: 0.1361 - acc: 0.9560 - val_loss: 0.3605 - val_acc: 0.9063\n",
      "Epoch 886/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.1402 - acc: 0.9514 - val_loss: 0.3584 - val_acc: 0.9014\n",
      "Epoch 887/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.1429 - acc: 0.9511 - val_loss: 0.3504 - val_acc: 0.9075\n",
      "Epoch 888/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.1328 - acc: 0.9617 - val_loss: 0.3510 - val_acc: 0.9100\n",
      "Epoch 889/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.1334 - acc: 0.9554 - val_loss: 0.3549 - val_acc: 0.9106\n",
      "Epoch 890/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.1325 - acc: 0.9529 - val_loss: 0.3579 - val_acc: 0.9051\n",
      "Epoch 891/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.1510 - acc: 0.9478 - val_loss: 0.3642 - val_acc: 0.9100\n",
      "Epoch 892/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.1390 - acc: 0.9584 - val_loss: 0.3534 - val_acc: 0.9081\n",
      "Epoch 893/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.1463 - acc: 0.9502 - val_loss: 0.3625 - val_acc: 0.9094\n",
      "Epoch 894/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.1457 - acc: 0.9520 - val_loss: 0.3439 - val_acc: 0.9100\n",
      "Epoch 895/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.1486 - acc: 0.9487 - val_loss: 0.3732 - val_acc: 0.9069\n",
      "Epoch 896/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.1251 - acc: 0.9620 - val_loss: 0.3609 - val_acc: 0.9051\n",
      "Epoch 897/1000\n",
      "3315/3315 [==============================] - 2s 547us/step - loss: 0.1400 - acc: 0.9544 - val_loss: 0.3785 - val_acc: 0.8953\n",
      "Epoch 898/1000\n",
      "3315/3315 [==============================] - 2s 549us/step - loss: 0.1364 - acc: 0.9551 - val_loss: 0.3537 - val_acc: 0.9088\n",
      "Epoch 899/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.1384 - acc: 0.9544 - val_loss: 0.3639 - val_acc: 0.9039\n",
      "Epoch 900/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.1465 - acc: 0.9487 - val_loss: 0.3531 - val_acc: 0.9094\n",
      "Epoch 901/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.1527 - acc: 0.9517 - val_loss: 0.3494 - val_acc: 0.9063\n",
      "Epoch 902/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.1289 - acc: 0.9578 - val_loss: 0.3551 - val_acc: 0.9032\n",
      "Epoch 903/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.1251 - acc: 0.9581 - val_loss: 0.3681 - val_acc: 0.9081\n",
      "Epoch 904/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.1403 - acc: 0.9508 - val_loss: 0.3510 - val_acc: 0.9051\n",
      "Epoch 905/1000\n",
      "3315/3315 [==============================] - 2s 530us/step - loss: 0.1393 - acc: 0.9563 - val_loss: 0.3518 - val_acc: 0.9057\n",
      "Epoch 906/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.1341 - acc: 0.9548 - val_loss: 0.3334 - val_acc: 0.9143\n",
      "Epoch 907/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.1399 - acc: 0.9548 - val_loss: 0.3718 - val_acc: 0.9075\n",
      "Epoch 908/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 0.1520 - acc: 0.9529 - val_loss: 0.3538 - val_acc: 0.9088\n",
      "Epoch 909/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.1387 - acc: 0.9535 - val_loss: 0.3343 - val_acc: 0.9094\n",
      "Epoch 910/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.1586 - acc: 0.9490 - val_loss: 0.3415 - val_acc: 0.9112\n",
      "Epoch 911/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 0.1527 - acc: 0.9484 - val_loss: 0.3387 - val_acc: 0.9100\n",
      "Epoch 912/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.1240 - acc: 0.9590 - val_loss: 0.3603 - val_acc: 0.9118\n",
      "Epoch 913/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.1311 - acc: 0.9575 - val_loss: 0.3512 - val_acc: 0.9039\n",
      "Epoch 914/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.1379 - acc: 0.9544 - val_loss: 0.3471 - val_acc: 0.9106\n",
      "Epoch 915/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.1412 - acc: 0.9544 - val_loss: 0.3462 - val_acc: 0.9124\n",
      "Epoch 916/1000\n",
      "3315/3315 [==============================] - 2s 530us/step - loss: 0.1395 - acc: 0.9526 - val_loss: 0.3396 - val_acc: 0.9161\n",
      "Epoch 917/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.1368 - acc: 0.9578 - val_loss: 0.3455 - val_acc: 0.9118\n",
      "Epoch 918/1000\n",
      "3315/3315 [==============================] - 2s 548us/step - loss: 0.1540 - acc: 0.9487 - val_loss: 0.3565 - val_acc: 0.9118\n",
      "Epoch 919/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.1280 - acc: 0.9541 - val_loss: 0.3398 - val_acc: 0.9124\n",
      "Epoch 920/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 0.1332 - acc: 0.9578 - val_loss: 0.3486 - val_acc: 0.9088\n",
      "Epoch 921/1000\n",
      "3315/3315 [==============================] - 2s 530us/step - loss: 0.1375 - acc: 0.9514 - val_loss: 0.3414 - val_acc: 0.9118\n",
      "Epoch 922/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.1274 - acc: 0.9605 - val_loss: 0.3507 - val_acc: 0.9130\n",
      "Epoch 923/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 0.1285 - acc: 0.9548 - val_loss: 0.3436 - val_acc: 0.9124\n",
      "Epoch 924/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.1279 - acc: 0.9569 - val_loss: 0.3493 - val_acc: 0.9106\n",
      "Epoch 925/1000\n",
      "3315/3315 [==============================] - 2s 526us/step - loss: 0.1406 - acc: 0.9505 - val_loss: 0.3366 - val_acc: 0.9130\n",
      "Epoch 926/1000\n",
      "3315/3315 [==============================] - 2s 547us/step - loss: 0.1310 - acc: 0.9575 - val_loss: 0.3652 - val_acc: 0.9045\n",
      "Epoch 927/1000\n",
      "3315/3315 [==============================] - 2s 555us/step - loss: 0.1457 - acc: 0.9508 - val_loss: 0.3244 - val_acc: 0.9155\n",
      "Epoch 928/1000\n",
      "3315/3315 [==============================] - 2s 526us/step - loss: 0.1284 - acc: 0.9602 - val_loss: 0.3351 - val_acc: 0.9130\n",
      "Epoch 929/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.1370 - acc: 0.9557 - val_loss: 0.3537 - val_acc: 0.9124\n",
      "Epoch 930/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.1319 - acc: 0.9584 - val_loss: 0.3544 - val_acc: 0.9124\n",
      "Epoch 931/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.1291 - acc: 0.9587 - val_loss: 0.3643 - val_acc: 0.9118\n",
      "Epoch 932/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.1332 - acc: 0.9526 - val_loss: 0.3340 - val_acc: 0.9155\n",
      "Epoch 933/1000\n",
      "3315/3315 [==============================] - 2s 530us/step - loss: 0.1295 - acc: 0.9578 - val_loss: 0.3457 - val_acc: 0.9075\n",
      "Epoch 934/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 0.1334 - acc: 0.9560 - val_loss: 0.3514 - val_acc: 0.9100\n",
      "Epoch 935/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.1352 - acc: 0.9563 - val_loss: 0.3531 - val_acc: 0.9081\n",
      "Epoch 936/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.1161 - acc: 0.9584 - val_loss: 0.3441 - val_acc: 0.9143\n",
      "Epoch 937/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.1360 - acc: 0.9538 - val_loss: 0.3826 - val_acc: 0.8996\n",
      "Epoch 938/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.1258 - acc: 0.9608 - val_loss: 0.3467 - val_acc: 0.9100\n",
      "Epoch 939/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.1296 - acc: 0.9575 - val_loss: 0.3406 - val_acc: 0.9112\n",
      "Epoch 940/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.1318 - acc: 0.9578 - val_loss: 0.3451 - val_acc: 0.9112\n",
      "Epoch 941/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.1218 - acc: 0.9617 - val_loss: 0.3478 - val_acc: 0.9069\n",
      "Epoch 942/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.1281 - acc: 0.9572 - val_loss: 0.3454 - val_acc: 0.9100\n",
      "Epoch 943/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.1307 - acc: 0.9538 - val_loss: 0.3662 - val_acc: 0.9045\n",
      "Epoch 944/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.1306 - acc: 0.9538 - val_loss: 0.3737 - val_acc: 0.9051\n",
      "Epoch 945/1000\n",
      "3315/3315 [==============================] - 2s 522us/step - loss: 0.1309 - acc: 0.9557 - val_loss: 0.3532 - val_acc: 0.9106\n",
      "Epoch 946/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 0.1218 - acc: 0.9584 - val_loss: 0.3549 - val_acc: 0.9137\n",
      "Epoch 947/1000\n",
      "3315/3315 [==============================] - 2s 549us/step - loss: 0.1323 - acc: 0.9551 - val_loss: 0.3591 - val_acc: 0.9100\n",
      "Epoch 948/1000\n",
      "3315/3315 [==============================] - 2s 584us/step - loss: 0.1204 - acc: 0.9569 - val_loss: 0.3490 - val_acc: 0.9112\n",
      "Epoch 949/1000\n",
      "3315/3315 [==============================] - 2s 582us/step - loss: 0.1301 - acc: 0.9569 - val_loss: 0.3596 - val_acc: 0.9106\n",
      "Epoch 950/1000\n",
      "3315/3315 [==============================] - 2s 571us/step - loss: 0.1210 - acc: 0.9596 - val_loss: 0.3573 - val_acc: 0.9088\n",
      "Epoch 951/1000\n",
      "3315/3315 [==============================] - 2s 559us/step - loss: 0.1364 - acc: 0.9581 - val_loss: 0.3550 - val_acc: 0.9094\n",
      "Epoch 952/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 0.1269 - acc: 0.9532 - val_loss: 0.3405 - val_acc: 0.9075\n",
      "Epoch 953/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.1238 - acc: 0.9578 - val_loss: 0.3639 - val_acc: 0.9045\n",
      "Epoch 954/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.1387 - acc: 0.9554 - val_loss: 0.3643 - val_acc: 0.9026\n",
      "Epoch 955/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.1294 - acc: 0.9578 - val_loss: 0.3641 - val_acc: 0.9094\n",
      "Epoch 956/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.1292 - acc: 0.9569 - val_loss: 0.3373 - val_acc: 0.9186\n",
      "Epoch 957/1000\n",
      "3315/3315 [==============================] - 2s 528us/step - loss: 0.1398 - acc: 0.9575 - val_loss: 0.3446 - val_acc: 0.9186\n",
      "Epoch 958/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.1135 - acc: 0.9578 - val_loss: 0.3661 - val_acc: 0.9057\n",
      "Epoch 959/1000\n",
      "3315/3315 [==============================] - 2s 530us/step - loss: 0.1241 - acc: 0.9578 - val_loss: 0.3481 - val_acc: 0.9124\n",
      "Epoch 960/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.1244 - acc: 0.9581 - val_loss: 0.3692 - val_acc: 0.9081\n",
      "Epoch 961/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.1270 - acc: 0.9587 - val_loss: 0.3758 - val_acc: 0.8983\n",
      "Epoch 962/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.1260 - acc: 0.9575 - val_loss: 0.3583 - val_acc: 0.9075\n",
      "Epoch 963/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.1321 - acc: 0.9569 - val_loss: 0.3446 - val_acc: 0.9118\n",
      "Epoch 964/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.1312 - acc: 0.9599 - val_loss: 0.3528 - val_acc: 0.9045\n",
      "Epoch 965/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.1138 - acc: 0.9635 - val_loss: 0.3599 - val_acc: 0.9112\n",
      "Epoch 966/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.1402 - acc: 0.9514 - val_loss: 0.3510 - val_acc: 0.9051\n",
      "Epoch 967/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.1211 - acc: 0.9590 - val_loss: 0.3655 - val_acc: 0.9094\n",
      "Epoch 968/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.1297 - acc: 0.9581 - val_loss: 0.3525 - val_acc: 0.9137\n",
      "Epoch 969/1000\n",
      "3315/3315 [==============================] - 2s 539us/step - loss: 0.1335 - acc: 0.9554 - val_loss: 0.3594 - val_acc: 0.9106\n",
      "Epoch 970/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.1266 - acc: 0.9551 - val_loss: 0.3314 - val_acc: 0.9143\n",
      "Epoch 971/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.1377 - acc: 0.9538 - val_loss: 0.3847 - val_acc: 0.8996\n",
      "Epoch 972/1000\n",
      "3315/3315 [==============================] - 2s 547us/step - loss: 0.1158 - acc: 0.9611 - val_loss: 0.3343 - val_acc: 0.9204\n",
      "Epoch 973/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.1216 - acc: 0.9569 - val_loss: 0.3425 - val_acc: 0.9094\n",
      "Epoch 974/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.1321 - acc: 0.9569 - val_loss: 0.3610 - val_acc: 0.9112\n",
      "Epoch 975/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 0.1223 - acc: 0.9620 - val_loss: 0.3393 - val_acc: 0.9155\n",
      "Epoch 976/1000\n",
      "3315/3315 [==============================] - 2s 524us/step - loss: 0.1095 - acc: 0.9623 - val_loss: 0.3435 - val_acc: 0.9094\n",
      "Epoch 977/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 0.1196 - acc: 0.9638 - val_loss: 0.3401 - val_acc: 0.9112\n",
      "Epoch 978/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 0.1140 - acc: 0.9602 - val_loss: 0.3697 - val_acc: 0.9014\n",
      "Epoch 979/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.1427 - acc: 0.9502 - val_loss: 0.3600 - val_acc: 0.9057\n",
      "Epoch 980/1000\n",
      "3315/3315 [==============================] - 2s 530us/step - loss: 0.1234 - acc: 0.9551 - val_loss: 0.3406 - val_acc: 0.9063\n",
      "Epoch 981/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.1142 - acc: 0.9569 - val_loss: 0.3388 - val_acc: 0.9149\n",
      "Epoch 982/1000\n",
      "3315/3315 [==============================] - 2s 533us/step - loss: 0.1360 - acc: 0.9514 - val_loss: 0.3471 - val_acc: 0.9155\n",
      "Epoch 983/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 0.1351 - acc: 0.9520 - val_loss: 0.3568 - val_acc: 0.9149\n",
      "Epoch 984/1000\n",
      "3315/3315 [==============================] - 2s 529us/step - loss: 0.1228 - acc: 0.9578 - val_loss: 0.3364 - val_acc: 0.9143\n",
      "Epoch 985/1000\n",
      "3315/3315 [==============================] - 2s 535us/step - loss: 0.1208 - acc: 0.9599 - val_loss: 0.3312 - val_acc: 0.9173\n",
      "Epoch 986/1000\n",
      "3315/3315 [==============================] - 2s 532us/step - loss: 0.1162 - acc: 0.9638 - val_loss: 0.3475 - val_acc: 0.9081\n",
      "Epoch 987/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.1210 - acc: 0.9602 - val_loss: 0.3531 - val_acc: 0.9112\n",
      "Epoch 988/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.1206 - acc: 0.9614 - val_loss: 0.3620 - val_acc: 0.9014\n",
      "Epoch 989/1000\n",
      "3315/3315 [==============================] - 2s 538us/step - loss: 0.1018 - acc: 0.9662 - val_loss: 0.3781 - val_acc: 0.9032\n",
      "Epoch 990/1000\n",
      "3315/3315 [==============================] - 2s 534us/step - loss: 0.1316 - acc: 0.9541 - val_loss: 0.3452 - val_acc: 0.9118\n",
      "Epoch 991/1000\n",
      "3315/3315 [==============================] - 2s 530us/step - loss: 0.1270 - acc: 0.9578 - val_loss: 0.3610 - val_acc: 0.9130\n",
      "Epoch 992/1000\n",
      "3315/3315 [==============================] - 2s 531us/step - loss: 0.1357 - acc: 0.9514 - val_loss: 0.3375 - val_acc: 0.9100\n",
      "Epoch 993/1000\n",
      "3315/3315 [==============================] - 2s 548us/step - loss: 0.1129 - acc: 0.9650 - val_loss: 0.3557 - val_acc: 0.9149\n",
      "Epoch 994/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.1176 - acc: 0.9593 - val_loss: 0.3587 - val_acc: 0.9081\n",
      "Epoch 995/1000\n",
      "3315/3315 [==============================] - 2s 530us/step - loss: 0.1230 - acc: 0.9581 - val_loss: 0.3447 - val_acc: 0.9106\n",
      "Epoch 996/1000\n",
      "3315/3315 [==============================] - 2s 537us/step - loss: 0.1218 - acc: 0.9569 - val_loss: 0.3674 - val_acc: 0.9069\n",
      "Epoch 997/1000\n",
      "3315/3315 [==============================] - 2s 548us/step - loss: 0.1090 - acc: 0.9644 - val_loss: 0.3659 - val_acc: 0.9026\n",
      "Epoch 998/1000\n",
      "3315/3315 [==============================] - 2s 545us/step - loss: 0.1150 - acc: 0.9617 - val_loss: 0.3575 - val_acc: 0.9094\n",
      "Epoch 999/1000\n",
      "3315/3315 [==============================] - 2s 536us/step - loss: 0.1148 - acc: 0.9629 - val_loss: 0.3587 - val_acc: 0.9112\n",
      "Epoch 1000/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.1270 - acc: 0.9578 - val_loss: 0.3683 - val_acc: 0.9112\n"
     ]
    }
   ],
   "source": [
    "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=1000, validation_data=(x_testcnn, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFytY6LDzgJ0"
   },
   "source": [
    "Let's plot the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "TFz4ClZov9gZ",
    "outputId": "e3fdf6e2-f249-4b36-a063-683c88ab705f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnnhistory.history['loss'])\n",
    "plt.plot(cnnhistory.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vf1W7LgP2DA5"
   },
   "source": [
    "\n",
    "\n",
    "And now let's plot the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "8yyFBt7ASPUe",
    "outputId": "d149ff38-7f2f-4eb4-d62e-08d8683ede2d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8v+1lxfX4aX3LQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnnhistory.history['acc'])\n",
    "plt.plot(cnnhistory.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gaZONl1mD8XD"
   },
   "source": [
    "Let's now create a classification report to review the f1-score of the model per class.\n",
    "To do so, we have to:\n",
    "- Create a variable predictions that will contain the model.predict_classes outcome\n",
    "- Convert our y_test (array of strings with our classes) to an array of int called new_Ytest, otherwise it will not be comparable to the predictions by the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EO25uIL-9vqx"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(x_testcnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1i06grlBBSrn",
    "outputId": "af34893b-827c-4355-a92a-17b53b80ad3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 3, ..., 1, 4, 3])"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HUHshx93CM_6",
    "outputId": "5b33758e-9a1a-403d-9679-19c0e6a2c0e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 3, ..., 1, 4, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tMxojpvWCxOs"
   },
   "outputs": [],
   "source": [
    "new_Ytest = y_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W07EQaC8DE6i",
    "outputId": "9e7d7f0f-8cd3-4068-e42e-4e23c22f8a71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 3, ..., 1, 4, 0])"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_Ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FW2XHdTtEedk"
   },
   "source": [
    "Okay, now we can display the classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "IfVSRmMu96rC",
    "outputId": "34b4a0d1-1915-4f31-d105-456257932c16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       134\n",
      "           1       0.92      0.93      0.92       251\n",
      "           2       0.91      0.89      0.90       242\n",
      "           3       0.84      0.90      0.87       271\n",
      "           4       0.96      0.94      0.95       253\n",
      "           5       0.92      0.91      0.91       239\n",
      "           6       0.95      0.93      0.94       127\n",
      "           7       0.90      0.85      0.88       116\n",
      "\n",
      "    accuracy                           0.91      1633\n",
      "   macro avg       0.92      0.91      0.91      1633\n",
      "weighted avg       0.91      0.91      0.91      1633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(new_Ytest, predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hu1S5IowfSDG"
   },
   "source": [
    "And now, the confusion matrix: it will show us the misclassified samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "fdy09SCEd7Cl",
    "outputId": "4fd020f5-74c5-40e7-8b54-076c944901be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[122   4   0   8   0   0   0   0]\n",
      " [  1 234   4   8   0   2   2   0]\n",
      " [  0   8 216   5   3   6   0   4]\n",
      " [  6   5   2 245   2   6   0   5]\n",
      " [  2   2   4   3 237   2   2   1]\n",
      " [  0   0   4  18   0 217   0   0]\n",
      " [  0   0   4   2   2   0 118   1]\n",
      " [  0   2   4   2   4   3   2  99]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "matrix = confusion_matrix(new_Ytest, predictions)\n",
    "print (matrix)\n",
    "\n",
    "# 0 = neutral, 1 = calm, 2 = happy, 3 = sad, 4 = angry, 5 = fearful, 6 = disgust, 7 = surprised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x_ySPOyHxkZ3"
   },
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "f5kRmoD-sdHj",
    "outputId": "99ad6a5b-a4a6-42bc-ed78-229864c33d55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at /content/drive/My Drive/Ravdess_model/Emotion_Voice_Detection_Model.h5 \n"
     ]
    }
   ],
   "source": [
    "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
    "save_dir = '/content/drive/My Drive/Ravdess_model'\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MNUiznKNwUtJ"
   },
   "source": [
    "# Reloading the model to test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "T4oAv6Kx8RBE",
    "outputId": "7190c4e1-54a2-4b8e-c287-8f14167f0412"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 40, 128)           768       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5, 128)            82048     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 5128      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 87,944\n",
      "Trainable params: 87,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model = keras.models.load_model('/content/drive/My Drive/Ravdess_model/Emotion_Voice_Detection_Model.h5')\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FHtPzc0Y8hfZ"
   },
   "source": [
    "# Checking the accuracy of the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "qUi-Zjuf8hDB",
    "outputId": "531a3330-11ee-489b-9e61-ac4d60e43a84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1633/1633 [==============================] - 0s 140us/step\n",
      "Restored model, accuracy: 91.12%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = loaded_model.evaluate(x_testcnn, y_test)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8pXH3y7S9A1N"
   },
   "source": [
    "# Thank you for your attention! To be continued.."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EmotionsRecognition.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
